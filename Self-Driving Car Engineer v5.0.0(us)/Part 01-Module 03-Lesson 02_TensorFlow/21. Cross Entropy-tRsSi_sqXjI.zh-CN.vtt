WEBVTT
Kind: captions
Language: zh-CN

00:00:00.002 --> 00:00:03.961
独热编码 (one-hot encoding) 能适用于大多数情况

00:00:03.961 --> 00:00:06.842
但当你面对成千上万

00:00:06.842 --> 00:00:08.224
甚至上百万种不同类别的时候

00:00:08.224 --> 00:00:11.641
编码向量将无比巨大

00:00:11.641 --> 00:00:15.890
并且绝大部分都是 0 使编码效率变得极低

00:00:15.890 --> 00:00:19.840
接下来你会学习如何使用嵌入的方法来解决这个问题

00:00:19.840 --> 00:00:24.180
它的优势在于 只需要简单比较两个向量

00:00:24.180 --> 00:00:26.220
就能判断结果的优劣

00:00:26.220 --> 00:00:29.430
一个向量是分类器的输出 包含了当前样本属于每个类别的概率

00:00:29.430 --> 00:00:34.070
另外一个则是独热编码形式的标签向量

00:00:34.070 --> 00:00:36.030
我们来看看如何进行实际操作

00:00:36.030 --> 00:00:40.450
衡量两个向量距离 (差异) 的一个常用概念

00:00:40.450 --> 00:00:42.079
称为交叉熵 (Cross Entropy)

00:00:42.079 --> 00:00:44.795
这里我用 D 来表示这一距离

00:00:44.795 --> 00:00:47.354
数学上就是按照这个公式进行计算

00:00:47.354 --> 00:00:50.067
注意 交叉熵的运算不是对称运算

00:00:50.067 --> 00:00:51.582
因为这里有对数运算

00:00:51.582 --> 00:00:54.011
所以你要确保

00:00:54.011 --> 00:00:56.516
标签和输出的概率 分别在正确的位置上

00:00:56.516 --> 00:01:00.212
标签向量用独热编码的形式表示

00:01:00.212 --> 00:01:03.540
因此里面包含许多 0 无法对它们进行对数运算

00:01:03.540 --> 00:01:07.350
对于分类器输出的概率分布则是由 softmax 计算得到

00:01:07.350 --> 00:01:11.650
所以里面的每个数都大于 0 都能进行对数运算

00:01:11.650 --> 00:01:14.490
刚才讲了很多内容 我们来小结一下

00:01:14.490 --> 00:01:19.290
输入数据通过一个线性模型运算后 将被转换为对数几率

00:01:19.290 --> 00:01:22.800
运算很简单 即输入乘上一个矩阵 w 再加上一个偏置项 b

00:01:22.800 --> 00:01:26.244
这些对数几率 也就是得分

00:01:26.244 --> 00:01:29.205
可以通过 softmax 方法转化为概率

00:01:29.205 --> 00:01:32.803
紧接着我们通过交叉熵方法去比较

00:01:32.803 --> 00:01:35.330
输出的概率向量与经过独热编码的标签向量的差异

00:01:35.330 --> 00:01:39.310
整个过程被称为多项式逻辑回归分类法 (multinomial logistic classification)

