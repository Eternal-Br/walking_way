WEBVTT
Kind: captions
Language: ja-JP

00:00:00.014 --> 00:00:02.109
こうした技のいくつかは
すでに見てきました

00:00:02.109 --> 00:00:05.840
前に要求したのは 入力をゼロ平均
および等分散にすることでした

00:00:05.841 --> 00:00:07.958
これはSGDにとって非常に重要です

00:00:07.958 --> 00:00:11.868
比較的小さな分散を持つ
ランダムな重みで初期化することもお伝えしました

00:00:11.868 --> 00:00:14.140
これも同様です

00:00:14.140 --> 00:00:17.109
ここでは これらの重要な技を
さらにいくつか説明します

00:00:17.109 --> 00:00:21.109
SGDの実装について本当に気にかける
必要があることをすべてカバーします

00:00:21.109 --> 00:00:23.480
まずはモメンタムです

00:00:23.480 --> 00:00:28.160
すでに述べたように 各ステップで
非常に小さいステップをランダムな方向に取ります

00:00:29.199 --> 00:00:33.130
しかし 合計すると それらのステップは
損失の最小化に向かっています

00:00:33.130 --> 00:00:35.820
これまでのステップから蓄積してきた
知識を活用できます

00:00:35.820 --> 00:00:38.740
どこに向かうべきかに関する
知識です

00:00:39.850 --> 00:00:43.550
安上がりな方法は
勾配の移動平均を維持して

00:00:43.549 --> 00:00:46.779
その移動平均を現在の一群の
データの方向の代わりに

00:00:46.780 --> 00:00:48.329
使うことです

00:00:48.329 --> 00:00:52.019
このモメンタム手法はとてもよく機能し
たいていは より優れた収束につながります

00:00:53.189 --> 00:00:55.619
2つ目は学習率の減衰です

00:00:55.619 --> 00:01:00.129
すでに述べたように 勾配降下法をSGDに
置き換えると 目標までのステップが

00:01:00.130 --> 00:01:04.430
より小さく
よりノイズが多くなります

00:01:04.430 --> 00:01:06.610
ステップはどのくらい小さくなるでしょうか

00:01:06.609 --> 00:01:08.170
これも全体的な
研究分野です

00:01:09.290 --> 00:01:12.609
ただし
常に言えるのは

00:01:12.609 --> 00:01:16.000
トレーニングするときには
ステップを徐々に小さくするとよいということです。

00:01:16.000 --> 00:01:18.948
指数関数的減衰を
学習率に適用するようなものです

00:01:18.948 --> 00:01:22.552
損失が定常状態に近づくまで
より小さくしていきます

00:01:22.552 --> 00:01:24.849
これにはさまざまな方法が
ありますが

00:01:24.849 --> 00:01:27.768
時間とともに小さくしていくことが
鍵です

00:01:27.768 --> 00:01:27.817
[無音]

