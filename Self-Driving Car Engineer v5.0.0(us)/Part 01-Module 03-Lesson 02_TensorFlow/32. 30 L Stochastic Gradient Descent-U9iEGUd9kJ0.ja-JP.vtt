WEBVTT
Kind: captions
Language: ja-JP

00:00:00.000 --> 00:00:02.846
勾配降下法のスケーリングに関する問題は簡単です

00:00:02.846 --> 00:00:05.325
これらの勾配を計算する必要があります

00:00:05.325 --> 00:00:07.330
これは別の経験則です

00:00:07.330 --> 00:00:11.262
損失の計算にn個の浮動小数点演算がある場合

00:00:11.262 --> 00:00:14.875
その勾配の計算にはその約3倍の計算が必要です

00:00:14.875 --> 00:00:16.320
前に見たように

00:00:16.320 --> 00:00:18.115
この損失関数は巨大です

00:00:18.114 --> 00:00:21.500
トレーニングセットに含まれるすべての単一要素に依存します

00:00:21.500 --> 00:00:24.975
データセットが大きければ大量の計算が必要であり

00:00:24.975 --> 00:00:28.320
大量のデータをトレーニングできる必要があります

00:00:28.320 --> 00:00:30.929
なぜなら 現実の問題では常に

00:00:30.929 --> 00:00:32.509
使うデータが多いほど成果も多くなるからです

00:00:32.509 --> 00:00:34.695
勾配降下法は反復的であるため

00:00:34.695 --> 00:00:37.005
多くのステップに対して行う必要があります

00:00:37.005 --> 00:00:41.965
つまり データを何十回または何百回も処理することになります  これはよくありません

00:00:41.965 --> 00:00:43.485
代わりに ずるをします

00:00:43.484 --> 00:00:44.957
損失を計算する代わりに

00:00:44.957 --> 00:00:46.855
その推定を計算します

00:00:46.854 --> 00:00:48.449
非常に悪い推定です

00:00:48.450 --> 00:00:50.652
実際 ひどい推定です

00:00:50.652 --> 00:00:52.460
とてもひどい推定なので

00:00:52.460 --> 00:00:54.157
なぜうまくいくのか不思議に思うでしょう

00:00:54.156 --> 00:00:56.219
それは正しい感想です  なぜなら さらにましなものに

00:00:56.219 --> 00:00:59.640
するために 少し時間をかける必要があります

00:00:59.640 --> 00:01:02.460
使おうとしている推定では トレーニングデータの

00:01:02.460 --> 00:01:06.075
非常に小さいランダムな一部分に対する平均損失を単純に計算します

00:01:06.075 --> 00:01:10.784
毎回 1～1000のトレーニングサンプルを考えます

00:01:10.784 --> 00:01:12.944
ランダムという言葉を使ったのは それが非常に重要なためです

00:01:12.944 --> 00:01:15.944
サンプルの選択方法が十分にランダムではない場合

00:01:15.944 --> 00:01:18.034
この方法はうまくいかなくなります

00:01:18.034 --> 00:01:21.435
それでは これから トレーニングデータのごく一部分を取得し

00:01:21.435 --> 00:01:23.594
そのサンプルの損失を計算し

00:01:23.594 --> 00:01:26.257
そのサンプルの微分を計算して

00:01:26.257 --> 00:01:31.354
その微分が勾配降下法に使用するのに正しい方向であるふりをします

00:01:31.355 --> 00:01:33.660
まったく正しい方向ではありません

00:01:33.659 --> 00:01:34.950
実際

00:01:34.950 --> 00:01:37.725
実際の損失を減らすのではなく増やす場合さえあります

00:01:37.724 --> 00:01:40.289
しかし これを何回も何回も行い

00:01:40.290 --> 00:01:42.090
そのたびにきわめて小さいステップを使うことで

00:01:42.090 --> 00:01:44.730
補正します

00:01:44.730 --> 00:01:47.310
そのため 各ステップの計算負荷ははるかに小さくなりますが

00:01:47.310 --> 00:01:48.865
その代償が必要です

00:01:48.864 --> 00:01:52.879
1つの大きいステップではなく 多数の小さいステップを使用する必要があります

00:01:52.879 --> 00:01:55.664
とはいっても 結局は大きなメリットがあります

00:01:55.665 --> 00:01:57.525
実際 課題を見るとわかるように

00:01:57.525 --> 00:02:01.094
勾配降下法を行うより この方がはるかに効率的です

00:02:01.093 --> 00:02:03.884
この技法は確率的勾配降下法と呼ばれ

00:02:03.885 --> 00:02:05.978
ディープラーニングの核心部分です

00:02:05.977 --> 00:02:10.560
これは 確率的勾配降下法がデータとモデルサイズの両方のスケーリングによく対応できるためであり

00:02:10.560 --> 00:02:13.575
ここではビッグデータとビッグモデルの両方が必要です

00:02:13.575 --> 00:02:16.185
確率的勾配降下法(SGD)は

00:02:16.185 --> 00:02:17.689
優れたスケーラブルな技法です

00:02:17.689 --> 00:02:20.504
しかし 基本的にはかなり悪いオプティマイザーであり

00:02:20.504 --> 00:02:23.579
十分に速い方法がたまたまこれしかないから使うのであって

00:02:23.580 --> 00:02:25.610
実際には多くの問題が発生します

