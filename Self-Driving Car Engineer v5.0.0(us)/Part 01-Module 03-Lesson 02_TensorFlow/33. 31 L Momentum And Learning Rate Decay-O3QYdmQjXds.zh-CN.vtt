WEBVTT
Kind: captions
Language: zh-CN

00:00:00.014 --> 00:00:02.109
你已经学过其中的一些技巧

00:00:02.109 --> 00:00:05.840
我之前要求过 让输入的均值为零、方差相同

00:00:05.841 --> 00:00:07.958
这对 SGD 很重要

00:00:07.958 --> 00:00:11.868
我也告诉过大家 用方差较小的随机权重

00:00:11.868 --> 00:00:14.140
来进行初始化 是同一个道理

00:00:14.140 --> 00:00:17.109
下面我会讨论更多重要技巧

00:00:17.109 --> 00:00:21.109
其中基本包括了实现 SGD 所需要的所有技巧

00:00:21.109 --> 00:00:23.480
第一个是动量

00:00:23.480 --> 00:00:28.160
回忆一下 在 SGD 里 虽然我们每次只是往随机方向走一小步

00:00:29.199 --> 00:00:33.130
但累积起来 就能让我们找到损失函数的极小值

00:00:33.130 --> 00:00:35.820
我们能够充分利用先前经历的步长

00:00:35.820 --> 00:00:38.740
来判断我们找极小值的方向

00:00:39.850 --> 00:00:43.550
一个省事的方法 是追踪梯度的实时平均值

00:00:43.549 --> 00:00:46.779
用该值代替当前一批数据计算得出的方向

00:00:46.780 --> 00:00:48.329
用该值代替当前一批数据计算得出的方向

00:00:48.329 --> 00:00:52.019
这种动量技术很有效 常常会有更好的收敛性

00:00:53.189 --> 00:00:55.619
第二个 是学习速率衰减

00:00:55.619 --> 00:01:00.129
回忆一下 用 SGD 代替梯度下降时 我说过

00:01:00.130 --> 00:01:04.430
在寻找目标的过程中 我们的每一步步长要走得越来越小、噪声越来越高

00:01:04.430 --> 00:01:06.610
步长要多小呢？

00:01:06.609 --> 00:01:08.170
这其实也是个研究领域

00:01:09.290 --> 00:01:12.609
不过 步长随着训练的过程越来越小

00:01:12.609 --> 00:01:16.000
这样做总是有好处的

00:01:16.000 --> 00:01:18.948
有些人喜欢让学习速率按指数衰减 有些人喜欢

00:01:18.948 --> 00:01:22.552
每当损失函数衰减停滞时缩短步长

00:01:22.552 --> 00:01:24.849
方法很多 但要记住的关键一点

00:01:24.849 --> 00:01:27.768
是随着时间流逝而减小它的值

