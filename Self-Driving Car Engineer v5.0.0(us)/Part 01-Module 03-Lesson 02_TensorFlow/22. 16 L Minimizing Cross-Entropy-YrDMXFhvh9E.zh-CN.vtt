WEBVTT
Kind: captions
Language: zh-CN

00:00:00.330 --> 00:00:03.010
好了 现在我们对问题的各个方面有了一个大致的了解

00:00:03.010 --> 00:00:07.080
这节课的问题 是我们该如何确定权重项 w

00:00:07.080 --> 00:00:11.880
和偏差项 b 的值 以使我们的分类器能够完成我们所期望的工作

00:00:11.880 --> 00:00:15.760
也就是说 当正确分类时，距离要足够小

00:00:15.760 --> 00:00:19.400
当错误分类时，距离要足够大

00:00:19.400 --> 00:00:22.290
你可以选择通过这样的方式衡量

00:00:22.290 --> 00:00:28.340
求出对所有训练集样本和所有现有标签的距离之和

00:00:28.340 --> 00:00:30.240
这个和被称为训练损失函数

00:00:30.240 --> 00:00:33.840
这个损失函数 求出了对于所有训练集样本的交叉熵的均值

00:00:33.840 --> 00:00:37.140
是一个非常庞大的函数

00:00:37.140 --> 00:00:40.800
训练集中的每个样本 都将被乘上这个巨大的 W 矩阵

00:00:40.800 --> 00:00:44.190
并且将它们相加求和 得到一个很大的总和值

00:00:45.240 --> 00:00:48.020
我们希望每一个距离值都很小 也就是说

00:00:48.020 --> 00:00:51.600
分类器能够 对训练集中的每个样本都能很好地分类

00:00:51.600 --> 00:00:53.130
因而我们希望这个训练损失函数值很小

00:00:54.220 --> 00:00:56.890
损失函数是一个关于权重项 w 和偏差项 b 的函数

00:00:56.890 --> 00:00:59.140
那我们来看看 如何求损失函数最小值

00:01:00.160 --> 00:01:02.730
假设该损失函数只有两个权重系数

00:01:02.730 --> 00:01:04.560
为了方便起见 

00:01:04.560 --> 00:01:05.970
我们记为 w1 和 w2

00:01:05.970 --> 00:01:09.670
该函数的值在某些区域很大

00:01:09.670 --> 00:01:11.200
在另一些区域很小

00:01:11.200 --> 00:01:15.540
我们的目标 是去寻找使得损失函数值最小的权重值

00:01:15.540 --> 00:01:18.210
因而 我们将一个机器学习问题

00:01:18.210 --> 00:01:20.770
转化为了一个值优化问题

00:01:20.770 --> 00:01:24.390
而我们有许多的方法来解决这个值优化问题

00:01:24.390 --> 00:01:29.060
最简单的方法之一 也许你可能遇到过 是梯度下降法

00:01:29.060 --> 00:01:32.260
对损失函数的每一个变量求偏导数

00:01:32.260 --> 00:01:35.635
并将每个变量值加上该偏导值

00:01:35.635 --> 00:01:37.890
进行迭代 直到你达到全局最小值

00:01:37.890 --> 00:01:40.760
梯度下降法相对而言比较简单

00:01:40.760 --> 00:01:44.310
特别是你能够帮你高效求出偏导数的值工具的时候

00:01:44.310 --> 00:01:46.370
请注意 我只给你展示了

00:01:46.370 --> 00:01:50.630
一个二元函数的偏导数的计算过程 但对于一个一般的问题

00:01:50.630 --> 00:01:54.350
我们可能面对的是一个有成百上千参数的函数

