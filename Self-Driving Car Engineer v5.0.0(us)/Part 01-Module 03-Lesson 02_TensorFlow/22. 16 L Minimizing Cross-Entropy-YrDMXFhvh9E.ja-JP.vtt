WEBVTT
Kind: captions
Language: ja-JP

00:00:00.330 --> 00:00:03.010
さあ これで パズルの
ピースが 揃いました

00:00:03.010 --> 00:00:07.080
問題は 重みwと
バイアスbを 適切に設定して

00:00:07.080 --> 00:00:11.880
クラシファイ（識別器）を 思いどおりに機能させるには
どうすればいいか です

00:00:11.880 --> 00:00:15.760
要するに 正しいクラスの
距離を 短く

00:00:15.760 --> 00:00:19.400
正しくないクラスの
距離を 長くしたいわけです

00:00:19.400 --> 00:00:23.290
1つの 方法として 距離を
すべての入力値と すべてのラベルに対し

00:00:23.290 --> 00:00:28.340
トレーニングセット全体で
平均することができます

00:00:28.340 --> 00:00:30.240
これを トレーニング損失と いいます

00:00:30.240 --> 00:00:33.840
この損失は トレーニングセット
全体で 平均した クロスエントロピーであり

00:00:33.840 --> 00:00:37.140
1つの 大きな関数です。

00:00:37.140 --> 00:00:40.800
トレーニングセット内の すべての例に
この大きな 行列Wが かけられます

00:00:40.800 --> 00:00:44.190
その後 すべての値を
足して 1つの大きな和を 求めます

00:00:45.240 --> 00:00:48.020
すべての距離が 小さければ
トレーニングデータ内の

00:00:48.020 --> 00:00:51.600
例の分類が うまく機能していることを
意味します

00:00:51.600 --> 00:00:53.130
つまり 損失を 小さくしたいわけです

00:00:54.220 --> 00:00:56.890
損失は 重みと
バイアスの 関数です

00:00:56.890 --> 00:00:59.140
そこで この関数の
最小化を 試みます

00:01:00.160 --> 00:01:02.730
損失が 2つの重みの
関数であると しましょう

00:01:02.730 --> 00:01:04.560
重み1と 重み2です

00:01:04.560 --> 00:01:05.970
これは 便宜的につけた 名前です

00:01:05.970 --> 00:01:09.670
この関数は 
一部の領域では 大きく

00:01:09.670 --> 00:01:11.200
他の領域では 小さくなります

00:01:11.200 --> 00:01:15.540
この損失が 最小になるような 重みを
試してみましょう

00:01:15.540 --> 00:01:18.210
これで マシーンラーニングの問題が
数値最適化の問題に

00:01:18.210 --> 00:01:20.770
変換されました

00:01:20.770 --> 00:01:24.390
数値最適化の問題を 解決する方法は
たくさん あります

00:01:24.390 --> 00:01:29.060
最も単純なのは 皆さん きっと
聞いたことがある 勾配降下法です

00:01:29.060 --> 00:01:32.260
パラメーターに関して
損失の微分係数を 求め

00:01:32.260 --> 00:01:35.635
微分係数に従って 1ステップ 戻る
処理を 繰り返し

00:01:35.635 --> 00:01:37.890
一番下に 到達します

00:01:37.890 --> 00:01:40.760
勾配降下法は 比較的 単純です
微分係数を 計算してくれる

00:01:40.760 --> 00:01:44.310
強力な数値ツールがあると
さらに 簡単になります

00:01:44.310 --> 00:01:46.370
ここで 扱っている 微分係数は
2つのパラメーターを持つ 関数の ものですが

00:01:46.370 --> 00:01:50.630
通常の 問題では
数千 数百万 あるいは

00:01:50.630 --> 00:01:54.350
数十億の
パラメーターを持つ 関数も あります

