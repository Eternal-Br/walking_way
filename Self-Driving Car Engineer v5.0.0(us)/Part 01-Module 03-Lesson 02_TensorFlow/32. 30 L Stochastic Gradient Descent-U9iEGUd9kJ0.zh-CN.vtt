WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.846
梯度下降的规模所带来的问题很简单

00:00:02.846 --> 00:00:05.325
你需要计算这些梯度

00:00:05.325 --> 00:00:07.330
而另一个经验法则是

00:00:07.330 --> 00:00:11.262
如果计算损失函数需要 n 次浮点运算

00:00:11.262 --> 00:00:14.875
计算其梯度则大约需要 3 倍运算量

00:00:14.875 --> 00:00:16.320
正如我们之前看到的

00:00:16.320 --> 00:00:18.115
最后一个函数非常庞大

00:00:18.114 --> 00:00:21.500
它与训练集中每一个元素都是相关的

00:00:21.500 --> 00:00:24.975
如果你的数据集很大 这可能需要很大的计算量

00:00:24.975 --> 00:00:28.320
我们当然希望有大量数据进行训练

00:00:28.320 --> 00:00:30.929
在实际情况中

00:00:30.929 --> 00:00:32.509
使用的数据越多越好

00:00:32.509 --> 00:00:34.695
因为梯度下降法是迭代算法

00:00:34.695 --> 00:00:37.005
你需要多次重复这些步骤

00:00:37.005 --> 00:00:41.965
这意味着所有数据要经历几十甚至上百次计算  这并不理想

00:00:41.965 --> 00:00:43.485
因此我们打算采用一些取巧的办法

00:00:43.484 --> 00:00:44.957
与计算损失不同

00:00:44.957 --> 00:00:46.855
我们来估计损失

00:00:46.854 --> 00:00:48.449
它是一个非常差的估计

00:00:48.450 --> 00:00:50.652
实际上它可以说是差到家了

00:00:50.652 --> 00:00:52.460
这个估计如此糟糕

00:00:52.460 --> 00:00:54.157
以至于你会奇怪 究竟它为什么会起作用

00:00:54.156 --> 00:00:56.219
你也许想到了

00:00:56.219 --> 00:00:59.640
因为我们会想办法去改进它

00:00:59.640 --> 00:01:02.460
我们将使用的估计只是简单计算 

00:01:02.460 --> 00:01:06.075
随机选取的一小部分训练数据的平均损失

00:01:06.075 --> 00:01:10.784
每次处理一到一千个训练样本

00:01:10.784 --> 00:01:12.944
我强调随机选取 因为这一点是非常重要的 

00:01:12.944 --> 00:01:15.944
如果你选择样本的方式不够随机

00:01:15.944 --> 00:01:18.034
该方法就不再奏效 

00:01:18.034 --> 00:01:21.435
我们选取一小部分的训练数据

00:01:21.435 --> 00:01:23.594
计算它们的损失

00:01:23.594 --> 00:01:26.257
计算它们的导数

00:01:26.257 --> 00:01:31.354
并认为这个导数的方向是梯度下降的正确方向。

00:01:31.355 --> 00:01:33.660
虽然实际上它并不总是正确的方向

00:01:33.659 --> 00:01:34.950
事实上 有时

00:01:34.950 --> 00:01:37.725
它甚至可能使损失增加 而不是减少

00:01:37.724 --> 00:01:40.289
因此我们需要许多许多次这样的计算

00:01:40.290 --> 00:01:42.090
来进行补偿

00:01:42.090 --> 00:01:44.730
每次计算都是非常小的一步

00:01:44.730 --> 00:01:47.310
所以每步计算的负担就减小了很多

00:01:47.310 --> 00:01:48.865
但我们还是付出了代价

00:01:48.864 --> 00:01:52.879
我们必须进行多得多的小步骤 而不能一步到位

00:01:52.879 --> 00:01:55.664
总而言之 我们还是赢了

00:01:55.665 --> 00:01:57.525
事实上 正如你将在作业中看到的那样

00:01:57.525 --> 00:02:01.094
这样做比逐个样本进行梯度下降更有效率

00:02:01.093 --> 00:02:03.884
这种方法被称为随机梯度下降

00:02:03.885 --> 00:02:05.978
它是深度学习的核心知识点

00:02:05.977 --> 00:02:10.560
这是因为随机梯度下降能够适应各种数据和模型的大小

00:02:10.560 --> 00:02:13.575
我们希望大数据和大型模型

00:02:13.575 --> 00:02:16.185
随机梯度下降 简称 SGD (Stochastic Gradient Descent)

00:02:16.185 --> 00:02:17.689
是一种优秀的 可扩展的方法

00:02:17.689 --> 00:02:20.504
不过 由于它本质上是一个糟糕的优化器

00:02:20.504 --> 00:02:23.579
却又恰恰是唯一足够快的优化器

00:02:23.580 --> 00:02:25.610
它存在许多需要注意的问题

