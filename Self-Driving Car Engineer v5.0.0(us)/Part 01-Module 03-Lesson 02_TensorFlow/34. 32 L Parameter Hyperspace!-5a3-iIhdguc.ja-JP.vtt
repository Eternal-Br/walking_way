WEBVTT
Kind: captions
Language: ja-JP

00:00:00.260 --> 00:00:02.849
学習率のトレーニングは
奇妙に見えることがあります

00:00:02.850 --> 00:00:05.679
たとえば 学習率が高いほど
学習量が増えたり

00:00:05.679 --> 00:00:10.279
学習速度が上がったりすると考えがちですが
実際はそうではありません

00:00:10.279 --> 00:00:14.289
実際多くの場合 モデルを採用し
学習率を下げると

00:00:14.289 --> 00:00:16.390
モデルをより短期間で改良できます

00:00:16.390 --> 00:00:17.850
さらに悪いことがあります

00:00:17.850 --> 00:00:21.090
経時変化する損失の曲線を見れば
どれほど速く学習できるか確認できると

00:00:21.089 --> 00:00:22.998
期待しているかもしれません

00:00:24.160 --> 00:00:26.399
高い学習率では改善がすぐに見られますが
その後横ばいになります

00:00:26.399 --> 00:00:31.299
一方 低い学習率は
改善が継続します

00:00:31.300 --> 00:00:35.439
ニューラルネットワークのトレーニング経験者には
見慣れた図です

00:00:35.439 --> 00:00:37.369
学習速度は多くの場合

00:00:37.369 --> 00:00:39.739
トレーニングの質とは
ほとんど関係ありません

00:00:40.969 --> 00:00:44.560
これが SGDが黒魔術と呼ばれる
理由です

00:00:44.560 --> 00:00:48.350
試しに使うことのできる
ハイパーパラメーターは多数あります

00:00:48.350 --> 00:00:51.910
初期化パラメーター、
学習率パラメーター、減衰、モメンタムなどを使用できますが

00:00:51.909 --> 00:00:54.889
きちんと理解しておく必要があります

00:00:54.890 --> 00:00:59.289
実際にはさほど悪くはありません
うまくいかないときには

00:00:59.289 --> 00:01:03.769
必ず最初に学習率を
下げてみてください

00:01:03.770 --> 00:01:06.150
小さいモデルには
優れたソリューションが多数あります

00:01:06.150 --> 00:01:09.840
しかし残念なことに現状では
実際に扱うレベルの大規模モデルには

00:01:09.840 --> 00:01:11.909
完璧なソリューションが
存在しません

00:01:13.060 --> 00:01:17.640
ADAGRADというアプローチがあり
これが少し役に立ちます

00:01:17.640 --> 00:01:21.680
ADAGRADは
SGDの修正版であり

00:01:21.680 --> 00:01:23.630
モメンタムと学習率の減衰を自動的に行います

00:01:23.629 --> 00:01:28.144
ADAGRADを使用すると 多くの場合
学習へのハイパーパラメーターの影響が低減します

00:01:28.144 --> 00:01:29.100
[無音]

