WEBVTT
Kind: captions
Language: zh-CN

00:00:00.260 --> 00:00:02.849
学习速率调节有时候难以琢磨

00:00:02.850 --> 00:00:05.679
例如 你可能觉得用更高的学习速率

00:00:05.679 --> 00:00:10.279
会学得更多或更快 但并非如此

00:00:10.279 --> 00:00:14.289
事实上 你通常拿到一个模型 降低它的学习速率

00:00:14.289 --> 00:00:16.390
可以更快地得到一个更好的模型

00:00:16.390 --> 00:00:17.850
情况更糟糕的是

00:00:17.850 --> 00:00:21.090
你总有一种冲动 通过观察误差曲线的随时间递减的情况

00:00:21.089 --> 00:00:22.998
来判断学习速度快慢

00:00:24.160 --> 00:00:26.399
看这里 较高的学习速率刚开始学得比较快

00:00:26.399 --> 00:00:31.299
随后便趋于平稳了 而低学习速率的曲线却在不断改善 训练得越来越好

00:00:31.300 --> 00:00:35.439
这对于训练过神经网络的人来说 是无比熟悉的一幕

00:00:35.439 --> 00:00:37.369
永远不要相信学得有多快

00:00:37.369 --> 00:00:39.739
它往往与训练得有多好关系甚微

00:00:40.969 --> 00:00:44.560
这也是为什么 SGD 获得了 “黑魔法” 的美誉

00:00:44.560 --> 00:00:48.350
你有许多超参数可供调整

00:00:48.350 --> 00:00:51.910
（权重）初始化参数、学习速率参数、学习速率衰减、

00:00:51.909 --> 00:00:54.889
动量 这些都要设置正确

00:00:54.890 --> 00:00:59.289
在实际操作中并没有那么糟糕 但有一点必须要记住的是

00:00:59.289 --> 00:01:03.769
如果毫无进展 首先尝试降低学习速率

00:01:03.770 --> 00:01:06.150
对较小的模型来说 已经有很多好的解决方案了

00:01:06.150 --> 00:01:09.840
但不幸的是 对于我们真正关心的较大的模型

00:01:09.840 --> 00:01:11.909
目前还没有任何完全满意的解决方案

00:01:13.060 --> 00:01:17.640
我提到过一个方法 叫 ADAGRAD 它还有点用

00:01:17.640 --> 00:01:21.680
ADAGRAD 是对 SGD 的改版 它已经自动为你选择了

00:01:21.680 --> 00:01:23.630
动量和学习速率衰减

00:01:23.629 --> 00:01:28.144
使用 ADAGRAD 通常会使学习过程对超参数不那么敏感

