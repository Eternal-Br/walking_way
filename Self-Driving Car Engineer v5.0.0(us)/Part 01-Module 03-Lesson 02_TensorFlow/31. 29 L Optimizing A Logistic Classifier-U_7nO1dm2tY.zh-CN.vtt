WEBVTT
Kind: captions
Language: zh-CN

00:00:00.520 --> 00:00:03.960
刚才的问题解决完了 让我们回到训练模型吧

00:00:03.960 --> 00:00:07.010
利用梯度下降法训练逻辑回归非常有效

00:00:07.010 --> 00:00:07.640
一方面

00:00:07.640 --> 00:00:10.900
你是直接优化你所关心的误差

00:00:10.900 --> 00:00:12.060
这一直是个非常棒的主意

00:00:12.060 --> 00:00:15.920
这就是为什么在应用时 许多机器学习的研究工作

00:00:15.920 --> 00:00:18.239
都是关于设计好的损失函数 用以最优化

00:00:19.480 --> 00:00:22.580
但正如你在作业中运行模型时一样

00:00:22.580 --> 00:00:23.840
你可能会遇到问题

00:00:23.840 --> 00:00:26.430
最大的问题 是样本规模化的问题

