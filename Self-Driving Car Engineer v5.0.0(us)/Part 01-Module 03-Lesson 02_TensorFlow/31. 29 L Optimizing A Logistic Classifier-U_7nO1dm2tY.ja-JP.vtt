WEBVTT
Kind: captions
Language: ja-JP

00:00:00.520 --> 00:00:03.960
それでは
トレーニングモデルに戻りましょう

00:00:03.960 --> 00:00:07.010
勾配降下法を使ったロジスティック回帰の
トレーニングは優れたものです

00:00:07.010 --> 00:00:07.640
たとえば

00:00:07.640 --> 00:00:10.900
使用する誤差測定を
直接最適化します

00:00:10.900 --> 00:00:12.060
これは常に優れたアイデアです

00:00:12.060 --> 00:00:15.920
そして これこそが実際に
多くのマシーンラーニング研究において

00:00:15.920 --> 00:00:18.239
最適化のための適切な損失関数の
設計が行われていることの理由です

00:00:19.480 --> 00:00:22.580
しかし 課題のモデルを実行したなら
お気づきかもしれませんが

00:00:22.580 --> 00:00:23.840
この方法にはいくつか問題があります

00:00:23.840 --> 00:00:26.430
最大の問題は
拡張が非常に困難なことです

