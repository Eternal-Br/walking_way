WEBVTT
Kind: captions
Language: ja-JP

00:00:00.000 --> 00:00:02.960
ここまでで データのロードと準備に関する作業を

00:00:02.960 --> 00:00:05.642
すべて行ってきました

00:00:05.642 --> 00:00:09.419
このセルで実際に 深層ニューラルネットワークを構築していきます

00:00:09.419 --> 00:00:12.214
まずはTensorFlowをロードします

00:00:12.214 --> 00:00:14.839
後でこのエポック変数を使って

00:00:14.839 --> 00:00:19.399
トレーニングデータをネットワーク経由で実行する回数をTensorFlowに指示します

00:00:19.399 --> 00:00:21.079
一般的に エポックが多ければ多いほど

00:00:21.079 --> 00:00:22.739
モデルのトレーニングは向上しますが

00:00:22.739 --> 00:00:25.054
トレーニング時間が長くなります

00:00:25.054 --> 00:00:28.160
後でバッチサイズ変数を使って

00:00:28.160 --> 00:00:33.204
一度にネットワークを通過するトレーニング画像の数をTensorFlowに指示します

00:00:33.204 --> 00:00:34.579
バッチサイズが大きければ大きいほど

00:00:34.579 --> 00:00:36.434
モデルのトレーニングは速くなりますが

00:00:36.435 --> 00:00:40.450
プロセッサーで実行できるバッチサイズにメモリー制限がある可能性があります

00:00:40.450 --> 00:00:42.859
ラボの中核をなす

00:00:42.859 --> 00:00:44.615
LeNetを使用します

00:00:44.615 --> 00:00:47.440
最初にハイパーパラメーターを設定します

00:00:47.439 --> 00:00:52.099
この場合 両方のハイパーパラメーターが 重みの初期化方法に関連します

00:00:52.100 --> 00:00:56.585
これらの値は試すことができ ここで行うよりもっとうまくいくかどうかを確かめることができます

00:00:56.585 --> 00:00:59.810
次に 最初の畳み込み層を構築します

00:00:59.810 --> 00:01:02.465
この層には

00:01:02.465 --> 00:01:04.174
5×5のフィルターがあり

00:01:04.174 --> 00:01:06.254
入力の深さは1 出力の深さは6です

00:01:06.254 --> 00:01:08.229
また バイアスを初期化してから

00:01:08.230 --> 00:01:13.442
conv2D関数を使って画像にフィルターを畳み込み

00:01:13.441 --> 00:01:15.764
最後にバイアスを追加します

00:01:15.765 --> 00:01:19.984
畳み込みの公式では

00:01:19.984 --> 00:01:22.385
出力の高さ=(入力の高さ-フィルターの高さ+1)÷垂直方向の

00:01:22.385 --> 00:01:25.500
ストライドとなっています

00:01:25.500 --> 00:01:29.284
この例では

00:01:29.284 --> 00:01:33.909
(32-5+1)÷1=28です

00:01:33.909 --> 00:01:36.729
この式は 出力幅についても同様に機能します

00:01:36.730 --> 00:01:39.160
出力幅も28です

00:01:39.159 --> 00:01:46.229
そのため 畳み込み層の出力は28×28×6です  これが最初の畳み込み層です

00:01:46.230 --> 00:01:49.715
次に 畳み込み層の出力をアクティブにします

00:01:49.715 --> 00:01:52.730
この例では ReLUアクティベーション関数を使います

00:01:52.730 --> 00:01:54.875
さらに 2x2のカーネルを

00:01:54.875 --> 00:01:58.099
2x2のストライドで出力をプールします

00:01:58.099 --> 00:02:02.799
これにより 14x14x6のプーリング出力を得られます

00:02:02.799 --> 00:02:07.864
その後 ネットワークは 別の畳み込みアクティベーション層と プーリング層のセットを通過し

00:02:07.864 --> 00:02:10.919
5×5×16の出力を得られます

00:02:10.919 --> 00:02:14.119
この出力を平坦化してベクターにします

00:02:14.120 --> 00:02:19.789
ベクターの長さは5×5×16 つまり400です

00:02:19.789 --> 00:02:22.370
このベクターを

00:02:22.370 --> 00:02:24.855
幅120の完全接続層に渡します

00:02:24.854 --> 00:02:29.864
その後 この完全接続層の出力にReLUアクティベーションを適用します

00:02:29.865 --> 00:02:34.930
今回は このパターンを層幅84で繰り返します

00:02:34.930 --> 00:02:37.990
最後に ラベルセットのクラス数に等しい幅で

00:02:37.990 --> 00:02:42.265
完全接続出力層をアタッチします

00:02:42.264 --> 00:02:44.589
この場合 各桁に1つずつで

00:02:44.590 --> 00:02:46.000
クラスは10あるので

00:02:46.000 --> 00:02:48.235
出力層は10です

00:02:48.235 --> 00:02:50.875
これらの出力はロジットとしても知られています

00:02:50.875 --> 00:02:54.150
これは LeNet関数から返されます

00:02:54.150 --> 00:02:57.425
これがLeNetのアーキテクチャー全体です

00:02:57.425 --> 00:02:59.960
今度はこれを使用します

