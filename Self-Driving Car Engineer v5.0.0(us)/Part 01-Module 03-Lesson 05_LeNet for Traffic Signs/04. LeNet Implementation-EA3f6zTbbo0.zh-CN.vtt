WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.960
目前为止 我们所做的只是

00:00:02.960 --> 00:00:05.642
数据加载和预处理

00:00:05.642 --> 00:00:09.419
在这个代码格中 我们开始正式搭建深度神经网络

00:00:09.419 --> 00:00:12.214
首先 我们导入 TensorFlow 库

00:00:12.214 --> 00:00:14.839
后面 我们会使用这个 EPOCHS (轮数) 变量

00:00:14.839 --> 00:00:19.399
来告诉 TensorFlow 需要在神经网络中运行多少次训练数据

00:00:19.399 --> 00:00:21.079
一般而言轮数越多

00:00:21.079 --> 00:00:22.739
模型的训练效果就越好

00:00:22.739 --> 00:00:25.054
但训练花费的时间也会越长

00:00:25.054 --> 00:00:28.160
后面 我们还会使用批量大小 (batch size) 这一变量

00:00:28.160 --> 00:00:33.204
来告诉 TensorFlow 每次在神经网络中运行的训练图像有多少个

00:00:33.204 --> 00:00:34.579
批量越大

00:00:34.579 --> 00:00:36.434
模型的训练速度就越快

00:00:36.435 --> 00:00:40.450
但每次运行的批量受到内存大小的限制

00:00:40.450 --> 00:00:42.859
接下来 我们看一下 LeNet 网络

00:00:42.859 --> 00:00:44.615
这是本次实验的核心

00:00:44.615 --> 00:00:47.440
首先 我们要设置更多的超参数

00:00:47.439 --> 00:00:52.099
在本例中 两个超参数都和如何初始化权重相关

00:00:52.100 --> 00:00:56.585
你可以尝试改变这些超参数 看看能否获得更好的结果

00:00:56.585 --> 00:00:59.810
然后 我们构建第一个卷积层

00:00:59.810 --> 00:01:02.465
该层有一个 5×5 大小的滤波器

00:01:02.465 --> 00:01:04.174
输入深度为 1

00:01:04.174 --> 00:01:06.254
输出深度为 6

00:01:06.254 --> 00:01:08.229
我们还需要对偏差进行初始化

00:01:08.230 --> 00:01:13.442
然后 我们使用 conv2D 函数对图像和滤波器进行卷积运算

00:01:13.441 --> 00:01:15.764
最后再加上偏差

00:01:15.765 --> 00:01:19.984
卷积公式告诉我们 输出高度等于

00:01:19.984 --> 00:01:22.385
输入高度减去滤波器高度 再加上 1 

00:01:22.385 --> 00:01:25.500
最后整体除以垂直方向的步长

00:01:25.500 --> 00:01:29.284
在这个例子中 具体值是 32 减去 5 加上 1 

00:01:29.284 --> 00:01:33.909
最后整体除以 1 等于 28

00:01:33.909 --> 00:01:36.729
这个公式同样适用于计算输出的宽度

00:01:36.730 --> 00:01:39.160
也等于 28

00:01:39.159 --> 00:01:46.229
因此 卷积层的输出大小就是 28×28×6 这是我们的第一个卷积层

00:01:46.230 --> 00:01:49.715
接下来 我们需要激活卷积层的输出

00:01:49.715 --> 00:01:52.730
在本例中 我们使用 ReLU 激活函数

00:01:52.730 --> 00:01:54.875
然后我们使用一个大小为 2×2 卷积核 并以 2×2 的步长

00:01:54.875 --> 00:01:58.099
对输出进行池化

00:01:58.099 --> 00:02:02.799
这样我们获得一个大小为 14×14×6 的池化输出

00:02:02.799 --> 00:02:07.864
然后 数据进入另一套卷积、激活和池化层中

00:02:07.864 --> 00:02:10.919
并得到了一个大小为 5×5×16 的输出

00:02:10.919 --> 00:02:14.119
然后 我们把该输出排成一列 得到一个向量

00:02:14.120 --> 00:02:19.789
该向量的长度为 5×5×16 也就是 400

00:02:19.789 --> 00:02:22.370
我们把该向量导入一个全连接层

00:02:22.370 --> 00:02:24.855
层的宽度为 120

00:02:24.854 --> 00:02:29.864
并对这个全连接层的输出使用 ReLU 激活函数

00:02:29.865 --> 00:02:34.930
重复上述过程 这次全连接层的宽度是 84

00:02:34.930 --> 00:02:37.990
最后 我们加入一个全连接输出层

00:02:37.990 --> 00:02:42.265
层的宽度等于标签集所包含的类别数

00:02:42.264 --> 00:02:44.589
在这个例子中 共有 10 个类别

00:02:44.590 --> 00:02:46.000
每一位对应一个类别

00:02:46.000 --> 00:02:48.235
所以输出层的宽度为 10

00:02:48.235 --> 00:02:50.875
这些输出也被称为对数几率 (logits)

00:02:50.875 --> 00:02:54.150
也就是从 LeNet 返回的结果

00:02:54.150 --> 00:02:57.425
这就是 LeNet 的整体架构

00:02:57.425 --> 00:02:59.960
接下去 我们来学习它的具体应用

