WEBVTT
Kind: captions
Language: ja-JP

00:00:00.000 --> 00:00:03.259
ここで TensorFlow変数を 設定します

00:00:03.259 --> 00:00:06.809
xは 入力バッチを 格納する プレースホルダーです

00:00:06.809 --> 00:00:09.689
バッチサイズの 初期値を Noneにすると

00:00:09.689 --> 00:00:13.414
プレースホルダーが どんなサイズのバッチでも 受け入れられるように なります

00:00:13.414 --> 00:00:19.780
画像の寸法を 32x32x1に 設定します  yには ラベルが 格納されます

00:00:19.780 --> 00:00:23.310
この例では ラベルは スパース変数を 渡します

00:00:23.309 --> 00:00:25.364
スパース変数とは 整数で

00:00:25.364 --> 00:00:27.314
まだ ワンホットエンコーディングされていません

00:00:27.315 --> 00:00:31.770
tf.one_hot関数を 使って ラベルの ワンホットエンコーディングを 行います

00:00:31.769 --> 00:00:34.439
次に トレーニングパイプラインを 設定します

00:00:34.439 --> 00:00:37.070
まず 別のハイパーパラメーターを 調整する 必要があります

00:00:37.070 --> 00:00:41.605
学習率は TensorFlowに対し ネットワークの 重みを 更新するペースを 知らせます

00:00:41.604 --> 00:00:44.809
0.001は デフォルト値として 悪くありませんが

00:00:44.810 --> 00:00:47.594
他の率なら どうなるかを 試してみても いいでしょう

00:00:47.594 --> 00:00:52.795
次に 入力データを LeNet関数に 渡して ロジットを 計算します

00:00:52.795 --> 00:00:57.170
tf.nn.softmax_cross_entropy_with_logits関数を

00:00:57.170 --> 00:00:59.450
これらのロジットを

00:00:59.450 --> 00:01:02.715
検証データラベルと比較し、クロスエントロピーを求めます

00:01:02.715 --> 00:01:05.480
クロスエントロピーはロジットが検証

00:01:05.480 --> 00:01:08.359
トレーニングラベルからどれだけ離れているかを示すに過ぎません

00:01:08.359 --> 00:01:13.924
tf.reduce_mean関数は すべてのトレーニング画像の クロスエントロピーを 平均します

00:01:13.924 --> 00:01:17.810
AdamOptimizerは Adamアルゴリズムを使って

00:01:17.810 --> 00:01:22.480
損失関数を 最小化します  確率的勾配降下法と 似ていますね

00:01:22.480 --> 00:01:27.344
Adamアルゴリズムは 確率的勾配降下法より 少し洗練されているので

00:01:27.344 --> 00:01:29.905
デフォルトの オプティマイザーに 適しています

00:01:29.905 --> 00:01:33.780
ここで 先ほど設定した 学習率ハイパーパラメーターを 使います

00:01:33.780 --> 00:01:37.409
最後に オプティマイザーに対して minimize関数を 実行します

00:01:37.409 --> 00:01:41.959
逆伝播で ネットワークが更新され トレーニング損失が 最小化されます

00:01:41.959 --> 00:01:43.759
これで トレーニングパイプラインが できました

00:01:43.760 --> 00:01:46.527
でも これは パイプラインに 過ぎないので

00:01:46.527 --> 00:01:50.010
機能させるためには 実際に データを 渡さなければなりません

00:01:50.010 --> 00:01:52.000
それは いくつかの コードセルで 実行します

