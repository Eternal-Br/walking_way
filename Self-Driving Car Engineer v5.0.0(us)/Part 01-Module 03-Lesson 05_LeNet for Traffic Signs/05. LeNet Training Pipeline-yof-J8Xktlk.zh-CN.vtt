WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.259
这一步 我们开始设置 TensorFlow 变量

00:00:03.259 --> 00:00:06.809
x 是占位变量 将用来存储一批输入数据

00:00:06.809 --> 00:00:09.689
我们将批量初始化为 None

00:00:09.689 --> 00:00:13.414
在后面的过程中 这个占位变量就可以接受任何批量大小的输入数据

00:00:13.414 --> 00:00:19.780
我们将图像尺寸设置为 32×32×1 标签存储在变量 y 中

00:00:19.780 --> 00:00:23.310
在这个例子中 标签是稀疏变量

00:00:23.309 --> 00:00:25.364
它们包含的都是整数

00:00:25.364 --> 00:00:27.314
也就是尚未进行独热编码 (one-hot encoding)

00:00:27.315 --> 00:00:31.770
我们使用 tf.one_hot 函数对标签进行独热编码处理

00:00:31.769 --> 00:00:34.439
现在 我们来设置训练流程

00:00:34.439 --> 00:00:37.070
首先 还有一个超参数需要调整

00:00:37.070 --> 00:00:41.605
学习速率决定了 TensorFlow 更新神经网络中权重的速度

00:00:41.604 --> 00:00:44.809
0.001 是个不错的默认值

00:00:44.810 --> 00:00:47.594
但你也可以尝试一下其他学习速率 看看效果如何

00:00:47.594 --> 00:00:52.795
接下来 我们把数据输入 LeNet 计算对数几率

00:00:52.795 --> 00:00:57.170
我们调用了 tf.nn.softmax_cross_entropy_with_logits 这个函数

00:00:57.170 --> 00:00:59.450
将对数几率结果和真实标签进行比较

00:00:59.450 --> 00:01:02.715
并计算交叉熵

00:01:02.715 --> 00:01:05.480
交叉熵用于测量对数几率的分类结果和

00:01:05.480 --> 00:01:08.359
真实标签之间的差距

00:01:08.359 --> 00:01:13.924
tf.reduce_mean 函数求出所有训练图像的交叉熵平均值

00:01:13.924 --> 00:01:17.810
AdamOptimizer 使用 Adam 算法来最小化损失函数

00:01:17.810 --> 00:01:22.480
这和随机梯度下降算法类似

00:01:22.480 --> 00:01:27.344
Adam 算法优化了随机梯度下降算法

00:01:27.344 --> 00:01:29.905
因此它也是优化算法的一个合适的默认选项

00:01:29.905 --> 00:01:33.780
这里的超参数是我们之前设置过的学习速率

00:01:33.780 --> 00:01:37.409
最后 我们在优化器上运行最小化函数

00:01:37.409 --> 00:01:41.959
通过反向传播更新神经网络 以最小化训练期间的损失函数值

00:01:41.959 --> 00:01:43.759
这就是我们的训练流程

00:01:43.760 --> 00:01:46.527
但目前为止 它只是一个流程框架而已

00:01:46.527 --> 00:01:50.010
我们需要输入数据 才能使其发挥作用

00:01:50.010 --> 00:01:52.000
我们将在接下来的几个代码格中介绍这部分内容

