WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.980
In the first code cell we load the MNIST data set

00:00:01.980 --> 00:00:05.515
which comes pre-installed with TensorFlow.

00:00:05.514 --> 00:00:09.160
Then we store the training validation and test sets.

00:00:09.160 --> 00:00:11.910
Next, we verify that the number of images in each set

00:00:11.910 --> 00:00:15.265
matches the number of labels in the same set.

00:00:15.265 --> 00:00:16.740
Then, we print out the shape of one image

00:00:16.739 --> 00:00:20.339
so that we know what the dimensions of the data are.

00:00:20.339 --> 00:00:23.429
Finally, we print out the size of each set.

00:00:23.429 --> 00:00:24.850
When we run the code,

00:00:24.850 --> 00:00:28.500
we see that the training set has 55,000 images.

00:00:28.500 --> 00:00:34.130
The validation set has 5,000 images and the test set has 10,000 images.

00:00:34.130 --> 00:00:37.570
In the next code cell we transformed the 28 by 28

00:00:37.570 --> 00:00:43.259
MNIST images into 32 by 32 images that LeNet can process.

00:00:43.259 --> 00:00:48.439
We could do this by using image processing software to scale up each image.

00:00:48.439 --> 00:00:52.159
But here, we just pad the images with zeroes around the edges.

00:00:52.159 --> 00:00:55.234
This is much faster and it works well.

00:00:55.234 --> 00:00:58.950
When we've done, the image shape is 32 by 32 by 1 

00:00:58.950 --> 00:01:02.710
which is exactly what LeNet takes as input.

00:01:02.710 --> 00:01:05.310
It's always a good idea to visualize our data 

00:01:05.310 --> 00:01:08.790
to make sure that everything actually looks the way we think it does.

00:01:08.790 --> 00:01:11.920
In this case, we select a random image 

00:01:11.920 --> 00:01:15.969
from the training set and we use matplotlib to visualize it.

00:01:15.969 --> 00:01:19.170
Then we also print out the label for that image.

00:01:19.170 --> 00:01:24.105
Happily, this label matches the image, one-one.

00:01:24.105 --> 00:01:26.410
In the next cell we preprocess the data

00:01:26.409 --> 00:01:30.489
which in this case just amounts to shuffling the training set.

00:01:30.489 --> 00:01:33.670
It's important to shuffle the training data otherwise 

00:01:33.670 --> 00:01:37.000
the ordering of the data might have a huge effect on how well the network trends.

