WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.960
Until this point, everything we've done has been

00:00:02.960 --> 00:00:05.642
related to loading and preparing the data.

00:00:05.642 --> 00:00:09.419
In this cell, we start actually building our deep neutral network.

00:00:09.419 --> 00:00:12.214
First, we load TensorFlow.

00:00:12.214 --> 00:00:14.839
Later we will use this EPOCHS variable,

00:00:14.839 --> 00:00:19.399
to tell TensorFlow how many times to run our training data through the network.

00:00:19.399 --> 00:00:21.079
In general, the more EPOCHS,

00:00:21.079 --> 00:00:22.739
the better our model will train,

00:00:22.739 --> 00:00:25.054
but also the longer training will take.

00:00:25.054 --> 00:00:28.160
Later we'll also use the batch size variable,

00:00:28.160 --> 00:00:33.204
to tell TensorFlow, how many training images to run through the network at a time.

00:00:33.204 --> 00:00:34.579
The larger the batch size,

00:00:34.579 --> 00:00:36.434
the faster our model will train,

00:00:36.435 --> 00:00:40.450
but our processor may have a memory limit on how large a batch it can run.

00:00:40.450 --> 00:00:42.859
Now we come to LeNet,

00:00:42.859 --> 00:00:44.615
which is the core of the lab.

00:00:44.615 --> 00:00:47.440
First we set more hyper parameters.

00:00:47.439 --> 00:00:52.099
In this case both hyper parameters relate to how we initialize our weights.

00:00:52.100 --> 00:00:56.585
You can experiment with these values and see if you can do better than what we have here.

00:00:56.585 --> 00:00:59.810
Then, we build the first convolutional layer.

00:00:59.810 --> 00:01:02.465
This layer has a five by five filter,

00:01:02.465 --> 00:01:04.174
with an input depth of one,

00:01:04.174 --> 00:01:06.254
and an output depth of six.

00:01:06.254 --> 00:01:08.229
We also initialize the bias,

00:01:08.230 --> 00:01:13.442
then we use the conv2D function to convolve the filter over the images,

00:01:13.441 --> 00:01:15.764
and we add the bias at the end.

00:01:15.765 --> 00:01:19.984
The formula for convolutions tells us that the output height equals,

00:01:19.984 --> 00:01:22.385
the input height minus the filter height,

00:01:22.385 --> 00:01:25.500
plus one all divided by the vertical stride.

00:01:25.500 --> 00:01:29.284
In this case, that means 32, minus 5,

00:01:29.284 --> 00:01:33.909
plus 2, all divided by 1, which equals 28.

00:01:33.909 --> 00:01:36.729
The formula works the same way for the output width,

00:01:36.730 --> 00:01:39.160
which also equals 28.

00:01:39.159 --> 00:01:46.229
So our convolutional layer output is 28 by 28 by 6.That's our first convolutional layer.

00:01:46.230 --> 00:01:49.715
Next, we activate the output of the convolutional layer,

00:01:49.715 --> 00:01:52.730
in this case with a ReLU activation function.

00:01:52.730 --> 00:01:54.875
Then we pool the output,

00:01:54.875 --> 00:01:58.099
using the 2 by 2 kernel with a 2 by 2 stride,

00:01:58.099 --> 00:02:02.799
which gives us a pooling output of 14 by 14 by 6.

00:02:02.799 --> 00:02:07.864
The network then runs through another set of convolutional activation and pooling layers,

00:02:07.864 --> 00:02:10.919
giving an output of 5 by 5 by 16.

00:02:10.919 --> 00:02:14.119
Then we flatten this output into a vector.

00:02:14.120 --> 00:02:19.789
The length of the vector is 5 times 5 times 16, which equals 400.

00:02:19.789 --> 00:02:22.370
We pass this vector into a fully connected layer,

00:02:22.370 --> 00:02:24.855
with a width of 120.

00:02:24.854 --> 00:02:29.864
Then we apply a ReLU activation to the output of this fully connected layer.

00:02:29.865 --> 00:02:34.930
We repeat that pattern again this time with a layer width of 84.

00:02:34.930 --> 00:02:37.990
Finally, we attach a fully connected output layer,

00:02:37.990 --> 00:02:42.265
with a width, equal to the number of classes in our label set.

00:02:42.264 --> 00:02:44.589
In this case,we have 10 classes,

00:02:44.590 --> 00:02:46.000
One for each digit,

00:02:46.000 --> 00:02:48.235
so the with the the output layer is 10.

00:02:48.235 --> 00:02:50.875
These outputs are also known as our logits,

00:02:50.875 --> 00:02:54.150
which is what we return from the LeNet function.

00:02:54.150 --> 00:02:57.425
So that is the entire LeNet architecture.

00:02:57.425 --> 00:02:59.960
Now we just have to put it to use.

