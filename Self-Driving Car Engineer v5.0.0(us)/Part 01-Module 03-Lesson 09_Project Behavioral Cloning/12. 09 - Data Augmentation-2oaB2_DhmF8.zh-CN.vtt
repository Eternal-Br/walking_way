WEBVTT
Kind: captions
Language: zh-CN

00:00:00.090 --> 00:00:01.950
我碰到了一个问题

00:00:01.950 --> 00:00:04.139
汽车似乎向左转向过度了

00:00:04.139 --> 00:00:05.639
这实际上是可以解释的

00:00:05.639 --> 00:00:07.529
训练赛道是一个圈

00:00:07.530 --> 00:00:09.359
汽车按逆时针方向行驶

00:00:09.359 --> 00:00:11.009
因此 在大多数时间里

00:00:11.009 --> 00:00:12.599
模型都在学习如何向左转向

00:00:12.599 --> 00:00:15.750
因此 在无人驾驶模式下 有时即使直行才是最佳选择

00:00:15.750 --> 00:00:18.839
模型还是会选择向左转向

00:00:18.839 --> 00:00:22.320
数据增强可在一定程度上解决这个问题

00:00:22.320 --> 00:00:25.050
有很多方式可以实现数据增强 以扩大训练集

00:00:25.050 --> 00:00:26.789
帮助模型更好地泛化

00:00:26.789 --> 00:00:28.649
我们可以改变图像亮度

00:00:28.649 --> 00:00:31.019
也可以水平或垂直移动图像

00:00:31.019 --> 00:00:34.289
本例中 我准备采用一种简单的方法

00:00:34.289 --> 00:00:36.479
水平反转图像

00:00:36.479 --> 00:00:37.649
就像照镜子一样

00:00:37.649 --> 00:00:39.248
接下来 我会将转向角的方向取反

00:00:39.249 --> 00:00:40.957
这样就可以获得一个平衡的数据集

00:00:40.957 --> 00:00:44.100
来训练车辆既能按顺时针

00:00:44.100 --> 00:00:46.050
也能按逆时针方向行驶

00:00:46.049 --> 00:00:49.439
和使用侧方摄像头的数据类似

00:00:49.439 --> 00:00:51.369
利用数据增强有两大好处

00:00:51.369 --> 00:00:54.539
其一 我们有更多数据可用于训练网络

00:00:54.539 --> 00:00:57.060
其二 我们用于训练网络的数据

00:00:57.060 --> 00:00:58.530
更加全面

00:00:58.530 --> 00:01:00.300
训练过程看上去一切顺利

00:01:00.299 --> 00:01:02.549
可以看到 每轮训练图像的数量

00:01:02.549 --> 00:01:04.469
是之前的两倍

00:01:04.469 --> 00:01:06.659
这是理所当然的 因为我复制了所有图像

00:01:06.659 --> 00:01:08.229
并把它们左右反转了

00:01:08.230 --> 00:01:10.140
然后看一下 模型表现如何

00:01:10.140 --> 00:01:11.549
好多了

00:01:11.549 --> 00:01:13.140
虽然离最后的成功还有不少距离

00:01:13.140 --> 00:01:15.950
但看上去这个模型还是进步了不少

