WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:02.940
Up to this point, I've only been using the center camera

00:00:02.940 --> 00:00:06.000
image, but now it might be helpful to use side camera

00:00:06.000 --> 00:00:07.620
images for training, too.

00:00:07.620 --> 00:00:10.900
Using the side camera images carries two benefits.

00:00:10.900 --> 00:00:12.960
One, it's simply more data.

00:00:12.960 --> 00:00:15.120
In fact, it's three times as much data.

00:00:15.119 --> 00:00:17.250
The other benefit is that using these images

00:00:17.250 --> 00:00:20.309
will help teach the network how to steer back to the center

00:00:20.309 --> 00:00:23.129
if the vehicle starts drifting off to the side.

00:00:23.129 --> 00:00:25.299
Let's look at the left image.

00:00:25.300 --> 00:00:28.230
This image is a little off to the left side of the road,

00:00:28.230 --> 00:00:30.810
but it's paired with a steering measurement of zero.

00:00:30.809 --> 00:00:32.879
That's because when this image was taken,

00:00:32.880 --> 00:00:36.040
the center of the vehicle really was in the center of the road.

00:00:36.039 --> 00:00:37.789
So how do I use this image?

00:00:37.789 --> 00:00:39.960
Basically, I want to train the network

00:00:39.960 --> 00:00:41.700
to steer a little harder to the right

00:00:41.700 --> 00:00:44.040
whenever the network sees an image like this.

00:00:44.039 --> 00:00:46.350
I'll do this by taking the actual steering

00:00:46.350 --> 00:00:48.789
measurement, which in this case is zero,

00:00:48.789 --> 00:00:51.810
and adding a small correction factor to it.

00:00:51.810 --> 00:00:53.609
I could actually draw out a model

00:00:53.609 --> 00:00:55.890
and use trigonometry and physics to calculate

00:00:55.890 --> 00:00:58.350
an optimal correction factor, but I'm just

00:00:58.350 --> 00:01:01.770
going to be lazy and try a correction factor of 0.2.

00:01:01.770 --> 00:01:04.129
Similarly, for the right camera image,

00:01:04.129 --> 00:01:06.420
I'm going to train the network to steer a little harder

00:01:06.420 --> 00:01:07.260
to the left.

00:01:07.260 --> 00:01:09.600
I'll do that by subtracting the correction factor

00:01:09.599 --> 00:01:11.159
from the actual steering measurement

00:01:11.159 --> 00:01:13.560
whenever I use a right camera image.

00:01:13.560 --> 00:01:15.900
I implement this by looping through the first three

00:01:15.900 --> 00:01:18.870
tokens of each line in the CSV file,

00:01:18.870 --> 00:01:21.990
and I use these to load each camera image into the images

00:01:21.989 --> 00:01:22.799
array.

00:01:22.799 --> 00:01:25.799
Then I add three measurements to the measurements array

00:01:25.799 --> 00:01:28.439
corresponding to each image I just added.

00:01:28.439 --> 00:01:30.659
Now I should have three times as much data,

00:01:30.659 --> 00:01:32.609
and my model should perform better.

00:01:32.609 --> 00:01:33.750
Let's see.

00:01:33.750 --> 00:01:35.159
It looks like the model is indeed

00:01:35.159 --> 00:01:38.099
training on three times as many camera images.

00:01:38.099 --> 00:01:41.989
I'm excited to see how well this drives.

00:01:41.989 --> 00:01:43.519
Ooh, it looks like it drives worse.

00:01:43.519 --> 00:01:46.089
It's just pulling to the left.

