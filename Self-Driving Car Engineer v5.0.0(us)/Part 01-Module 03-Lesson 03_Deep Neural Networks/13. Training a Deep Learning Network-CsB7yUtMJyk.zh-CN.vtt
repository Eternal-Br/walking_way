WEBVTT
Kind: captions
Language: zh-CN

00:00:00.370 --> 00:00:02.520
现在 你有一个“小”神经网络

00:00:02.520 --> 00:00:05.359
它不是很“深” 仅仅只有两层

00:00:05.360 --> 00:00:06.610
通过增大中间隐层的尺寸

00:00:06.610 --> 00:00:10.429
你可以让它变的更大、更复杂

00:00:10.429 --> 00:00:15.000
但总体上说 仅仅增大 H 数 并非高效的手段

00:00:15.000 --> 00:00:19.160
你需要让它变得非常非常大 以至于它很难被训练

00:00:19.160 --> 00:00:22.809
而这正是深度学习核心思想发挥作用的地方

00:00:22.809 --> 00:00:26.369
实际上 你可以通过增加更多层的方式 来使你的模型变的更“深”

00:00:26.370 --> 00:00:28.560
有如下原因

00:00:28.559 --> 00:00:30.559
一个是参数的“效率”

00:00:30.559 --> 00:00:34.908
通过让网络变深的方式 你可以通过增加较少的参数

00:00:34.908 --> 00:00:37.201
而使模型表现大为改善

00:00:37.201 --> 00:00:39.618
另一个原因是 这会出现很多

00:00:39.618 --> 00:00:41.310
神奇的现象

00:00:41.310 --> 00:00:45.917
更深模型 往往会呈现出层次化的结构

00:00:45.917 --> 00:00:47.539
我们以一个图形分类模型为例

00:00:47.539 --> 00:00:50.229
若我们画出它每层所学到的内容

00:00:50.229 --> 00:00:55.339
你会发现模型最底层会是一些非常简单的东西 例如线、边等

00:00:55.340 --> 00:00:56.440
再往右看

00:00:56.439 --> 00:00:59.820
则会有更复杂的东西出现 例如几何形状等

00:00:59.820 --> 00:01:03.640
再往深处看 你则会看到一些更具体的内容 例如脸、物件等

00:01:03.640 --> 00:01:07.188
该模型因此而显得十分强大 因为模型能够学习到

00:01:07.188 --> 00:01:10.368
数据中的抽象内容 而这正是你所期望的

00:01:10.367 --> 00:01:13.197
而最终 模型也会因此而更快地完成学习过程

