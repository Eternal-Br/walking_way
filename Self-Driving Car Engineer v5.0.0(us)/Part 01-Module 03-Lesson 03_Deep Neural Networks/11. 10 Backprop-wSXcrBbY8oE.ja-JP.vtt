WEBVTT
Kind: captions
Language: ja-JP

00:00:00.000 --> 00:00:02.240
例を 見てみましょう

00:00:02.240 --> 00:00:05.685
ネットワークが 単純な演算の スタックだとしましょう

00:00:05.684 --> 00:00:09.629
線形変換とか ReLUとか 何でもいいです

00:00:09.630 --> 00:00:13.575
行列変換のように パラメーターを 持つものもあれば

00:00:13.574 --> 00:00:15.674
ReLUのように 持たないものもあります

00:00:15.675 --> 00:00:18.030
入力のxに データを適用すると

00:00:18.030 --> 00:00:21.615
データは スタックの中を 通って 予測yに 到達します

00:00:21.614 --> 00:00:23.409
微分係数を 計算するために

00:00:23.410 --> 00:00:25.850
こんな感じの グラフを 別に作成します

00:00:25.850 --> 00:00:29.895
新しいグラフのデータは ネットワーク内を 逆方向に流れ

00:00:29.894 --> 00:00:34.784
先ほど見た 連鎖法則を使って 組み合わされ 勾配を生成します

00:00:34.784 --> 00:00:37.469
このグラフは ネットワーク内の 個々の演算から

00:00:37.469 --> 00:00:40.254
自動的に 導き出すことができます

00:00:40.255 --> 00:00:44.065
そのため ほとんどのディープラーニングフレームワークで 自動的に行われています

00:00:44.064 --> 00:00:48.195
これは 逆伝播という とても強力な 概念です

00:00:48.195 --> 00:00:51.725
複雑な関数の 微分係数の計算が とても効率的になります

00:00:51.725 --> 00:00:56.130
ただし 関数が 単純な微分係数を含む 単純なブロックで 構成されていることが 条件です

00:00:56.130 --> 00:01:00.315
予測まで モデルを実行するプロセスを 順伝播と いいます

00:01:00.314 --> 00:01:04.200
そして モデルを逆方向に 実行するプロセスを 逆伝播といいます

00:01:04.200 --> 00:01:07.225
まとめると 確率的勾配降下法を 実行するには

00:01:07.224 --> 00:01:11.184
トレーニングセットの データの バッチごとに

00:01:11.185 --> 00:01:14.745
順伝播と 逆伝播を 実行します

00:01:14.745 --> 00:01:19.219
これにより モデルの それぞれの重みの 勾配が 得られます

00:01:19.219 --> 00:01:21.539
次に 求めた勾配を 学習率に従って

00:01:21.540 --> 00:01:24.615
元の重みに 適用し 更新します

00:01:24.614 --> 00:01:28.224
そして この作業を 最初から 何度も何度も 繰り返します

00:01:28.224 --> 00:01:31.614
それにより モデルが 最適化されていきます

00:01:31.614 --> 00:01:35.819
それぞれの ブロックで どのような計算が 行われるかは 説明しません

00:01:35.819 --> 00:01:38.629
なぜなら 普通は 気にする必要がなく

00:01:38.629 --> 00:01:40.459
基本的には 連鎖法則だからです

00:01:40.459 --> 00:01:42.134
大切なのは この図です

00:01:42.135 --> 00:01:45.300
特に 逆伝播のブロックは 順伝播の

00:01:45.299 --> 00:01:50.314
2倍のメモリーを 必要とし 2倍量の計算を 行います

00:01:50.314 --> 00:01:54.000
このことは たとえば メモリーに合わせて モデルのサイズを決定するときに 重要です

