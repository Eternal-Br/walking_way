WEBVTT
Kind: captions
Language: ja-JP

00:00:00.370 --> 00:00:02.520
小さなニューラルネットワークがあります

00:00:02.520 --> 00:00:05.359
深さは特に深くなく
レイヤーは2つだけです

00:00:05.360 --> 00:00:06.610
中間の隠れレイヤーのサイズを拡大すると

00:00:06.610 --> 00:00:10.429
ニューラルネットワークをより大きく
複雑にすることができますが

00:00:10.429 --> 00:00:15.000
このHの増加は
一般的に効率向上につながりません

00:00:15.000 --> 00:00:19.160
Hはきわめて大きくする必要があるのですが
そうすると 今度はトレーニングが難しくなります

00:00:19.160 --> 00:00:22.809
そこでディープラーニングの考え方が
関連してきます

00:00:22.809 --> 00:00:26.369
代わりに 層をもっと追加して
モデルをより深くすることができます

00:00:26.370 --> 00:00:28.560
これを行う正当な理由は
多数あります

00:00:28.559 --> 00:00:30.559
その1つはパラメーターの効率です

00:00:30.559 --> 00:00:34.908
通常は 広くするよりも 深くすることにより
より少ないパラメーターで

00:00:34.908 --> 00:00:37.201
はるかに高いパフォーマンスを得られます

00:00:37.201 --> 00:00:39.618
もう1つは 人が関心を持つ
自然現象の多くは

00:00:39.618 --> 00:00:41.310
階層構造を持つ傾向があり

00:00:41.310 --> 00:00:45.917
深いモデルで取り込むのが
自然であるということです

00:00:45.917 --> 00:00:47.539
たとえば イメージのモデルを入力して

00:00:47.539 --> 00:00:50.229
モデルの学習内容を視覚化すると
多くの場合 レイヤーの最下部は

00:00:50.229 --> 00:00:55.339
線やエッジなど
非常に単純なものになります

00:00:55.340 --> 00:00:56.440
上に移動すると

00:00:56.439 --> 00:00:59.820
幾何学的形状のような
より複雑なものが見られる傾向があります

00:00:59.820 --> 00:01:03.640
さらに上に移動すると 顔のようなものが
見え始めます

00:01:03.640 --> 00:01:07.188
これは非常に強力です
なぜなら モデル構造が

00:01:07.188 --> 00:01:10.368
データに反映させたい抽象化の種類と
一致しているからです

00:01:10.367 --> 00:01:13.197
その結果として
モデルでの学習がより簡単になります

00:01:13.197 --> 00:01:15.068
[無音]

