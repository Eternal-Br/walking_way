WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.240
我们来看如下的例子

00:00:02.240 --> 00:00:05.685
假设你的神经网络 是由一系列的简单运算构成

00:00:05.684 --> 00:00:09.629
譬如线性变换、ReLu 等等

00:00:09.630 --> 00:00:13.575
有的变换有参数 如矩阵变换

00:00:13.574 --> 00:00:15.674
有的则没有 如 ReLu

00:00:15.675 --> 00:00:18.030
当你把数据输入到 x

00:00:18.030 --> 00:00:21.615
它们将会经过这堆运算 然后得到预测值 y

00:00:21.614 --> 00:00:23.409
为了计算相应的导数

00:00:23.410 --> 00:00:25.850
我们又画出了新的图来演示如何运算

00:00:25.850 --> 00:00:29.895
新图中数据的流动方向 与之前相反

00:00:29.894 --> 00:00:34.784
并通过我们刚刚演示的链式法则计算出梯度

00:00:34.784 --> 00:00:37.469
这个图 可以完全自动地从

00:00:37.469 --> 00:00:40.254
你神经网络中每一步的运算得出

00:00:40.255 --> 00:00:44.065
大多数深度学习框架 都会为我们完成这项工作

00:00:44.064 --> 00:00:48.195
这叫做反向传播 (Back-propagation) 是一个非常强大的概念

00:00:48.195 --> 00:00:51.725
当函数是由拥有简单导数的函数复合而成的时候

00:00:51.725 --> 00:00:56.130
该方法计算函数导数非常高效

00:00:56.130 --> 00:01:00.315
运行该模型进行预测的过程一般被称为正向传播 (Forward Prop)

00:01:00.314 --> 00:01:04.200
而当模型反向运行以计算导数的时候 则被称为反向传播 (Backward Prop)

00:01:04.200 --> 00:01:07.225
让我们来总结一下 在使用随机梯度下降法时

00:01:07.224 --> 00:01:11.184
对训练集中的每一小批数据

00:01:11.185 --> 00:01:14.745
你要先进行正向传播 然后进行反向传播

00:01:14.745 --> 00:01:19.219
如此 你将会得出你模型各个权重的更新梯度值

00:01:19.219 --> 00:01:21.539
然后 你就可以在原始的权重上

00:01:21.540 --> 00:01:24.615
使用求出的梯度值和学习率来更新它

00:01:24.614 --> 00:01:28.224
你将会重复该过程一次又一次

00:01:28.224 --> 00:01:31.614
而这 就是你的模型如何得到优化的

00:01:31.614 --> 00:01:35.819
我不准备展开讲 每一步运算背后的数学原理

00:01:35.819 --> 00:01:38.629
因为你通常不需要担心那些问题

00:01:38.629 --> 00:01:40.459
因为它的核心就是链式法则

00:01:40.459 --> 00:01:42.134
不过你要记住这个图

00:01:42.135 --> 00:01:45.300
特别是反向传播中的每个区块

00:01:45.299 --> 00:01:50.314
相比正向传播 它们通常需要花费两倍的算力与储存空间

00:01:50.314 --> 00:01:54.000
当你的内存相对较小 需要控制模型尺寸时 这将非常重要

