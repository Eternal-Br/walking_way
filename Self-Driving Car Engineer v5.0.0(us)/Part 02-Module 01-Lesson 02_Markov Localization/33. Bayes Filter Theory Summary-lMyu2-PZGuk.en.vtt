WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.240
Since I went
through a lot of math,

00:00:02.240 --> 00:00:05.660
I want to present the core
achievements of the last steps.

00:00:05.660 --> 00:00:07.570
So let's sum it up.

00:00:07.570 --> 00:00:09.910
The Bayes localization
filter, or Bayes filter,

00:00:09.910 --> 00:00:13.240
is a general framework for
recursive state estimation.

00:00:13.240 --> 00:00:16.210
Recursive means that we
use the previous state

00:00:16.210 --> 00:00:19.870
to estimate the new state by
using only current observations

00:00:19.870 --> 00:00:23.440
and controls, and not the
whole history of data.

00:00:23.440 --> 00:00:27.310
The motion model describes the
predictions step of the filter.

00:00:27.309 --> 00:00:29.899
The observation model
is the update step

00:00:29.899 --> 00:00:33.320
to estimate the new
state probabilities.

00:00:33.320 --> 00:00:36.320
You already heard about this
interaction between prediction

00:00:36.320 --> 00:00:38.520
and update step before.

00:00:38.520 --> 00:00:41.370
For example, when Andray
talked about common filters

00:00:41.369 --> 00:00:43.729
in the fusion lesson.

00:00:43.729 --> 00:00:47.599
This means the current 1D
localization, common filters,

00:00:47.600 --> 00:00:50.640
and also particle
filters are realizations

00:00:50.640 --> 00:00:52.579
of the Bayes filter.

00:00:52.579 --> 00:00:55.280
Now you understand probabilistic
reasoning, and recursive

00:00:55.280 --> 00:00:57.109
of state estimation.

00:00:57.109 --> 00:01:00.530
This is an amazing achievement,
not only for localization,

00:01:00.530 --> 00:01:04.310
but also for the whole
self-driving car nanodegree.

00:01:04.310 --> 00:01:07.480
Now that you know all
the theory behind it,

00:01:07.480 --> 00:01:12.370
let's go back to the C++ example
and finalize the localizer.

