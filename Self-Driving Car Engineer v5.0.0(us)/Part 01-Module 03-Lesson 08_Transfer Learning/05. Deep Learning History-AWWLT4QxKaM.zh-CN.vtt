WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.049
— 从 2006 年我读研究生时上的第一堂人工智能课开始

00:00:04.049 --> 00:00:09.875
我就一直在进行神经网络的研究工作 但神经网络研究的历史比这要长得多

00:00:09.875 --> 00:00:12.830
这段历史从 20 世纪 50 年代就开始了

00:00:12.830 --> 00:00:16.769
— 这一点非常重要 至少对我而言 神经网络最大的魅力之一

00:00:16.769 --> 00:00:21.375
就是经历多年积累 最后一夜成名

00:00:21.375 --> 00:00:25.740
早在上世纪 90 年代 Yann LeCun 就搭建了一个名为 LeNet 的神经网络

00:00:25.739 --> 00:00:29.009
当时的邮局和银行使用这个网络

00:00:29.010 --> 00:00:32.929
来识别邮件和支票上的数字 其重要性超乎想象

00:00:32.929 --> 00:00:34.679
但即便有了 LeNet 的成功

00:00:34.679 --> 00:00:37.634
人们对于神经网络仍然没有很浓厚的兴趣

00:00:37.634 --> 00:00:41.509
深度学习只是在过去的五年中才实现真正的飞跃

00:00:41.509 --> 00:00:43.765
实在是不明白为什么会花了这么久   

00:00:43.765 --> 00:00:46.170
— 是的 这个问题提的很好

00:00:46.170 --> 00:00:49.050
我想答案可以归结为

00:00:49.049 --> 00:00:50.679
可使用的被标记数据越来越多

00:00:50.679 --> 00:00:54.179
同时 现代处理器的计算能力也得到了大幅提升

00:00:54.179 --> 00:00:56.460
在过去很长一段时间内 我们都没有

00:00:56.460 --> 00:01:00.210
深度学习所需的大量标记数据集

00:01:00.210 --> 00:01:03.149
这些数据集的普及 主要得益于互联网的兴起

00:01:03.149 --> 00:01:07.989
这使得大型数据集的收集和标记成为可能

00:01:07.989 --> 00:01:09.929
但即使我们拥有了大量数据

00:01:09.930 --> 00:01:13.405
又往往没有足够的计算能力来使用它们

00:01:13.405 --> 00:01:16.320
直到最近五年 处理器已经变得足够大、足够快

00:01:16.319 --> 00:01:19.219
才使得训练大型神经网络成为可能

