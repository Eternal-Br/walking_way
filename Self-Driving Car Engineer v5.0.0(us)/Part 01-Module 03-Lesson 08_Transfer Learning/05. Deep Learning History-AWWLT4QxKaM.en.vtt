WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.049
&gt;&gt; So I've been working on neural networks since 2006 when I took

00:00:04.049 --> 00:00:09.875
my first AI class in grad school but neural networks actually have a much longer history.

00:00:09.875 --> 00:00:12.830
They go back all the way to the 1950s.

00:00:12.830 --> 00:00:16.769
&gt;&gt; That's a great point. One of the fascinating things about neural networks,

00:00:16.769 --> 00:00:21.375
at least to me, is how long they've taken to become an overnight success.

00:00:21.375 --> 00:00:25.740
As far back as the late 90s there was a network called the LeNet built by

00:00:25.739 --> 00:00:29.009
Yann LeCun that the post office and banks used to

00:00:29.010 --> 00:00:32.929
read digits on mail and checks and it was incredibly important.

00:00:32.929 --> 00:00:34.679
But even after LeNet,

00:00:34.679 --> 00:00:37.634
not a lot of interest picked up in neural networks.

00:00:37.634 --> 00:00:41.509
Deep learning has only really taken off in the last five years.

00:00:41.509 --> 00:00:43.765
It's kind of a puzzle why it took so long.

00:00:43.765 --> 00:00:46.170
&gt;&gt; Yeah, that's a good question.

00:00:46.170 --> 00:00:49.050
The answer boils down to the increased availability of

00:00:49.049 --> 00:00:50.679
label data along with

00:00:50.679 --> 00:00:54.179
the greatly increased computational throughput of modern processors.

00:00:54.179 --> 00:00:56.460
For a long time, we didn't have

00:00:56.460 --> 00:01:00.210
the huge label data sets that we needed to make deep learning work.

00:01:00.210 --> 00:01:03.149
Those data sets only became widely available with the rise of

00:01:03.149 --> 00:01:07.989
the Internet which made collecting and labeling huge data sets feasible.

00:01:07.989 --> 00:01:09.929
But even when we had big data sets,

00:01:09.930 --> 00:01:13.405
we often didn't have enough computational power to make use of them.

00:01:13.405 --> 00:01:16.320
It's only been in the last five years that processors have gotten

00:01:16.319 --> 00:01:19.219
big enough and fast enough to train large scale neural networks.

