<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   Bonus Round: Deep Learning [Optional]
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Transfer Learning
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Introduction.html">
       01. Introduction
      </a>
     </li>
     <li class="">
      <a href="02. Bryan Catanzaro.html">
       02. Bryan Catanzaro
      </a>
     </li>
     <li class="">
      <a href="03. GPU vs. CPU.html">
       03. GPU vs. CPU
      </a>
     </li>
     <li class="">
      <a href="04. Transfer Learning.html">
       04. Transfer Learning
      </a>
     </li>
     <li class="">
      <a href="05. Deep Learning History.html">
       05. Deep Learning History
      </a>
     </li>
     <li class="">
      <a href="06. ImageNet.html">
       06. ImageNet
      </a>
     </li>
     <li class="">
      <a href="07. AlexNet.html">
       07. AlexNet
      </a>
     </li>
     <li class="">
      <a href="08. AlexNet Today.html">
       08. AlexNet Today
      </a>
     </li>
     <li class="">
      <a href="09. VGG.html">
       09. VGG
      </a>
     </li>
     <li class="">
      <a href="10. Empirics.html">
       10. Empirics
      </a>
     </li>
     <li class="">
      <a href="11. GoogLeNet.html">
       11. GoogLeNet
      </a>
     </li>
     <li class="">
      <a href="12. ResNet.html">
       12. ResNet
      </a>
     </li>
     <li class="">
      <a href="13. Without Pre-trained Weights.html">
       13. Without Pre-trained Weights
      </a>
     </li>
     <li class="">
      <a href="14. Lab Transfer Learning.html">
       14. Lab: Transfer Learning
      </a>
     </li>
     <li class="">
      <a href="15. Outro.html">
       15. Outro
      </a>
     </li>
     <li class="">
      <a href="16. Bonus Round Deep Learning [Optional].html">
       16. Bonus Round: Deep Learning [Optional]
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          16. Bonus Round: Deep Learning [Optional]
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="additional-resources-on-deep-learning">
          Additional Resources on Deep Learning
         </h2>
         <p>
          Nice work reaching the end of the deep learning content! While you still have the project left to do here, we're also providing some additional resources and recent research on the topic that you can come back to if you have time later on.
         </p>
         <p>
          Reading research papers is a great way to get exposure to the latest and greatest in the field, as well as expand your learning. However, just like the project ahead, it's often best to
          <em>
           learn by doing
          </em>
          - if you find a paper that really excites you, try to implement it (or even something better) yourself!
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h5 id="optional-reading">
          Optional Reading
         </h5>
         <p>
          All of these are completely optional reading - you could spend hours reading through the entirety of these! We suggest moving onto the project first so you have what you’ve learned fresh on your mind, before coming back to check these out.
         </p>
         <p>
          We've categorized these papers to hopefully help you narrow down which ones might be of interest, as well as highlighted a couple key reads by category by including their
          <em>
           Abstract
          </em>
          section, which summarizes the paper.
         </p>
         <hr/>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h3 id="behavioral-cloning">
          Behavioral Cloning
         </h3>
         <p>
          The below paper shows one of the techniques Waymo has researched using imitation learning (aka behavioral cloning) to drive a car.
         </p>
         <p>
          <a href="https://arxiv.org/abs/1812.03079" rel="noopener noreferrer" target="_blank">
           ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst
          </a>
          by M. Bansal, A. Krizhevsky and A. Ogale
         </p>
         <blockquote>
          <p>
           <strong>
            Abstract:
           </strong>
           Our goal is to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. We find that standard behavior cloning is insufficient for handling complex driving scenarios, even when we leverage a perception system for preprocessing the input and a controller for executing the output on the car: 30 million examples are still not enough. We propose exposing the learner to synthesized data in the form of perturbations to the expert's driving, which creates interesting situations such as collisions and/or going off the road. Rather than purely imitating all data, we augment the imitation loss with additional losses that penalize undesirable events and encourage progress -- the perturbations then provide an important signal for these losses and lead to robustness of the learned model. We show that the ChauffeurNet model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of our proposed changes and show that the model is responding to the appropriate causal factors. Finally, we demonstrate the model driving a car in the real world.
          </p>
         </blockquote>
         <hr/>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h3 id="object-detection-and-tracking">
          Object Detection and Tracking
         </h3>
         <p>
          The below papers include various deep learning-based approaches to 2D and 3D object detection and tracking.
         </p>
         <p>
          <a href="https://arxiv.org/abs/1512.02325" rel="noopener noreferrer" target="_blank">
           SSD: Single Shot MultiBox Detector
          </a>
          by W. Liu, et. al.
         </p>
         <blockquote>
          <p>
           <strong>
            Abstract:
           </strong>
           We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. […] Experimental results […] confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. […]
          </p>
         </blockquote>
         <p>
          <a href="https://arxiv.org/abs/1711.06396" rel="noopener noreferrer" target="_blank">
           VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection
          </a>
          by Y. Zhou and O. Tuzel
         </p>
         <blockquote>
          <p>
           <strong>
            Abstract:
           </strong>
           Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. […] Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.
          </p>
         </blockquote>
         <p>
          <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Luo_Fast_and_Furious_CVPR_2018_paper.pdf" rel="noopener noreferrer" target="_blank">
           Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net
          </a>
          by W. Luo,
          <em>
           et. al.
          </em>
         </p>
         <blockquote>
          <p>
           <strong>
            Abstract:
           </strong>
           In this paper we propose a novel deep neural network that is able to jointly reason about 3D detection, tracking and motion forecasting given data captured by a 3D sensor. By jointly reasoning about these tasks, our holistic approach is more robust to occlusion as well as sparse data at range. Our approach performs 3D convolutions across space and time over a bird’s eye view representation of the 3D world, which is very efficient in terms of both memory and computation. Our experiments on a new very large scale dataset captured in several north american cities, show that we can outperform the state-of-the-art by a
           <br/>
           large margin. Importantly, by sharing computation we can perform all tasks in as little as 30 ms.
          </p>
         </blockquote>
         <hr/>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h3 id="semantic-segmentation">
          Semantic Segmentation
         </h3>
         <p>
          The below paper concerns a technique called semantic segmentation, where each pixel of an image gets classified individually!
         </p>
         <p>
          <a href="https://arxiv.org/abs/1511.00561" rel="noopener noreferrer" target="_blank">
           SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation
          </a>
          by V. Badrinarayanan, A. Kendall and R. Cipolla
         </p>
         <blockquote>
          <p>
           <strong>
            Abstract:
           </strong>
           We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. […] The novelty of SegNet lies in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance.
           <br/>
           […] We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. […]
          </p>
         </blockquote>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('16. Bonus Round: Deep Learning [Optional]')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
