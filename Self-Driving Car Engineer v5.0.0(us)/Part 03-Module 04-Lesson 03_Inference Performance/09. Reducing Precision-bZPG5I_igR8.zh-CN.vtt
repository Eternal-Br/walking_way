WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.120
神经网络通常以单精度进行训练

00:00:03.120 --> 00:00:05.304
32 位浮点数

00:00:05.304 --> 00:00:07.859
它有 23 位有效位数

00:00:07.860 --> 00:00:11.384
可以在科学学科中使用有效数字

00:00:11.384 --> 00:00:14.429
双精度或 64 位浮点数

00:00:14.429 --> 00:00:21.144
有 52 位有效数字 但结果表明 神经网络中不需要精度级别

00:00:21.144 --> 00:00:24.254
所以 大多数训练都是以单精度完成的

00:00:24.254 --> 00:00:27.954
但事实证明 我们甚至不需要单一的精度来完成所有任务

00:00:27.954 --> 00:00:30.239
尤其是像推理这样的东西

00:00:30.239 --> 00:00:36.795
使用半精度或16位 11位显著位于更低的位置

00:00:36.795 --> 00:00:39.645
导致准确度的最小变化

00:00:39.645 --> 00:00:42.965
但确实节省了时间和内存

00:00:42.965 --> 00:00:48.000
实质上 与32位相比 你可以将带宽增加16位

00:00:48.000 --> 00:00:49.799
将精度推向极限

00:00:49.799 --> 00:00:51.744
是一个活跃的研究领域

00:00:51.744 --> 00:00:56.419
事实上 一些结果已经发布 仅使用一个比特权重

00:00:56.420 --> 00:00:58.770
使用低精度训练神经网络的主要问题

00:00:58.770 --> 00:01:01.960
是对反向传播的影响

00:01:01.960 --> 00:01:04.754
精度较低造成的误差在通过整个网络后退计算的

00:01:04.754 --> 00:01:08.504
每个阶段都被放大

00:01:08.504 --> 00:01:11.699
不过这种后退计算只会发生在训练期间

00:01:11.700 --> 00:01:16.400
因此 对于推断这是一个伪命题 我们可以愉快地降低精度

00:01:16.400 --> 00:01:18.715
通常 我们将浮点值转换为

00:01:18.715 --> 00:01:23.150
256个唯一值的比特整数表示

00:01:23.150 --> 00:01:25.890
例如 022 55

00:01:25.890 --> 00:01:29.760
这从浮点到整数的精确度的具体降低

00:01:29.760 --> 00:01:33.700
引入了另外一种类型的优化

00:01:33.700 --> 00:01:38.895
将这样的值从连续范围约束到离散范围的过程

00:01:38.894 --> 00:01:41.099
被称为量化

