WEBVTT
Kind: captions
Language: ja

00:00:00.000 --> 00:00:04.000
この計画アルゴリズムを実装する
シンプルなコードを見てみましょう

00:00:04.000 --> 00:00:09.000
前回と同様ゼロと1を持つグリッドがあります
見覚えがありますね

00:00:09.000 --> 00:00:13.000
出発地は左上でゴールは右下です

00:00:13.000 --> 00:00:19.000
壁などの任意の障害物をここやここに配置でき

00:00:19.000 --> 00:00:23.000
ロボットは角で
S字カーブを描かざるを得なくなります

00:00:23.000 --> 00:00:29.000
この表でコードを実行すると
取得する表はこのようになります

00:00:29.000 --> 00:00:33.000
この状態はそれぞれ壁のない状態です

00:00:33.000 --> 00:00:35.000
この表から最適な動作が分かります

00:00:35.000 --> 00:00:40.000
ここは南に行けと言っています
ここは右へ そして上へ

00:00:40.000 --> 00:00:43.000
再び右、南へ行けと言っています

00:00:43.000 --> 00:00:47.000
絶対に到達しないであろう状態の
これらの位置でも関連する

00:00:47.000 --> 00:00:49.000
最適なポリシーと行動があります

00:00:49.000 --> 00:00:55.000
それは出発状態が存在せず
ゴール状態があるだけだからです

00:00:55.000 --> 00:01:00.000
初期状態の指定はこの結果に影響しません

00:01:00.000 --> 00:01:04.000
どうすればこれを効率良く計算できるでしょうか？

00:01:04.000 --> 00:01:09.000
簡単な例として
ここに障害物のある世界の例を挙げてみましょう

00:01:09.000 --> 00:01:12.000
ゴール状態はこの角だとします

00:01:12.000 --> 00:01:18.000
これらの各セルに動作を割り当てる最適な
ポリシーを計算する方法を説明するのではなく

00:01:18.000 --> 00:01:20.000
価値について説明しましょう

00:01:20.000 --> 00:01:24.000
価値関数は各グリッドセルに

00:01:24.000 --> 00:01:28.000
ゴールまでの最短経路の距離を関連づけます

00:01:28.000 --> 00:01:31.000
ゴールの場合は当然ゼロです

00:01:31.000 --> 00:01:34.000
ゴールの隣の各セルは1です

00:01:34.000 --> 00:01:44.000
ここは2、3、4、5、6、7です

00:01:44.000 --> 00:01:51.000
これは隣接する最適なx’とy’を取って
その価値を考慮して再帰的に計算されます

00:01:51.000 --> 00:01:56.000
そしてそこに行くまでのコストを加算します

00:01:56.000 --> 00:01:59.000
私たちの例ではプラス1になります

00:01:59.000 --> 00:02:02.000
この更新の方程式を再帰的に適用することで

00:02:02.000 --> 00:02:04.000
この価値関数を実現することができます

00:02:04.000 --> 00:02:11.000
この価値関数が得られれば最適な制御動作が
価値の最小化によって得られることが分かります

00:02:11.000 --> 00:02:15.000
これは山登りのような動作です

00:02:15.000 --> 00:02:18.000
では小テストを出しましょう

00:02:18.000 --> 00:02:22.000
この世界でゴールがここにある場合

00:02:22.000 --> 00:02:27.000
右下のセルの価値はいくつでしょうか？

