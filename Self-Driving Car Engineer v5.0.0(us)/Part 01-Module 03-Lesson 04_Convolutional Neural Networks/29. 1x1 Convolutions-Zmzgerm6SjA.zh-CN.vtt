WEBVTT
Kind: captions
Language: zh-CN

00:00:00.470 --> 00:00:03.209
首先 我要介绍一个概念

00:00:03.209 --> 00:00:05.000
即 1 x 1 卷积

00:00:06.030 --> 00:00:10.560
你可能在想 为什么会有人想用 1 x 1 卷积？

00:00:10.560 --> 00:00:13.900
因为它们关注的不是一块图像 而仅仅是一个像素

00:00:15.119 --> 00:00:19.859
让我们看一下传统的卷积 它基本上是运行在一小块图像上的

00:00:19.859 --> 00:00:23.429
小分类器 但仅仅是个线性分类器

00:00:24.879 --> 00:00:29.160
但如果你在中间加一个 1 x 1 卷积 你就用

00:00:29.160 --> 00:00:33.920
运行在一块图像上的神经网络代替了线性分类器

00:00:33.920 --> 00:00:38.832
在卷积操作中散布一些 1 x 1 卷积 是一种使模型变得更深

00:00:38.832 --> 00:00:42.296
的低耗高效的方法 并且会有更多的参数

00:00:42.296 --> 00:00:45.272
但未完全改变神经网络结构

00:00:45.273 --> 00:00:48.362
它们非常简单 因为如果你看它们的数学公式

00:00:48.362 --> 00:00:50.462
它们根本不是卷积

00:00:50.462 --> 00:00:54.563
只是矩阵相乘并且仅有较少的参数

00:00:54.563 --> 00:00:57.190
我之所以提及上述方法 平均池化和

00:00:57.189 --> 00:01:01.500
1 x 1 卷积 是因为我想讲一下在构建神经网络时

00:01:01.500 --> 00:01:05.337
非常成功的一般策略 相比于金字塔式的结构

00:01:05.337 --> 00:01:09.070
它们使得卷积网络更加的简洁高效

