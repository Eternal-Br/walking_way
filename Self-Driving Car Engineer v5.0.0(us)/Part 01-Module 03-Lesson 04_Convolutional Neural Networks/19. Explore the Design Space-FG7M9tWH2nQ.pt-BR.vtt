WEBVTT
Kind: captions
Language: pt-BR

00:00:00.000 --> 00:00:02.634
Agora que viram
uma ConvNet simples,

00:00:02.667 --> 00:00:05.300
podemos fazer muitas coisas
para melhorar.

00:00:05.334 --> 00:00:07.400
Vamos falar sobre 3:

00:00:07.434 --> 00:00:09.834
Agregação,
convolução 1x1,

00:00:09.868 --> 00:00:13.467
e algo mais avançado,
chamado inception.

00:00:13.501 --> 00:00:16.334
A primeira melhoria
é uma maneira de diminuir

00:00:16.367 --> 00:00:18.567
as dimensões espaciais
do mapa de características

00:00:18.601 --> 00:00:20.501
na pirâmide de convoluções.

00:00:20.534 --> 00:00:23.701
Até agora, usamos os passos
para mover os filtros

00:00:23.734 --> 00:00:25.200
alguns pixels a cada vez,

00:00:25.234 --> 00:00:27.634
e reduzir o tamanho
do mapa de características.

00:00:27.667 --> 00:00:30.667
É uma forma agressiva
de diminuir uma imagem.

00:00:30.701 --> 00:00:32.734
Remove muita informação.

00:00:32.767 --> 00:00:36.133
E se, em vez de pular
uma de cada duas convoluções,

00:00:36.167 --> 00:00:38.801
continuássemos com
um passo muito pequeno,

00:00:38.834 --> 00:00:39.968
talvez 1,

00:00:40.000 --> 00:00:43.334
mas fizéssemos todas as convoluções
em uma vizinhança

00:00:43.367 --> 00:00:46.234
e as combinássemos
de alguma forma?

00:00:46.267 --> 00:00:48.334
Essa operação
se chama agregação.

00:00:48.367 --> 00:00:50.100
Tem algumas formas
de fazer.

00:00:50.133 --> 00:00:52.400
A mais comum
é agregação por máximo.

00:00:52.434 --> 00:00:54.567
Em cada ponto
no mapa de características,

00:00:54.601 --> 00:00:57.267
olhe para uma vizinhança
pequena em torno do ponto

00:00:57.300 --> 00:01:00.400
e calcule o máximo
de todas as respostas ali.

00:01:00.434 --> 00:01:03.267
A agregação por máximo
tem algumas vantagens.

00:01:03.300 --> 00:01:06.400
Primeiro, ela não aumenta
o número de parâmetros.

00:01:06.434 --> 00:01:09.000
Não precisa aumentar
o sobre ajuste.

00:01:09.033 --> 00:01:12.434
Ela também costuma dar
modelos mais precisos.

00:01:12.467 --> 00:01:16.667
Mas, como as convoluções
usam um passo menor,

00:01:16.701 --> 00:01:20.133
o modelo se torna mais caro
para computar.

00:01:20.167 --> 00:01:22.901
Agora temos ainda mais
hiperparâmetros:

00:01:22.934 --> 00:01:25.000
o tamanho da região
de agregação,

00:01:25.033 --> 00:01:26.400
e o tamanho do passo.

00:01:26.434 --> 00:01:28.501
Não precisam ser iguais.

00:01:28.534 --> 00:01:31.000
Uma arquitetura típica
para uma ConvNet

00:01:31.033 --> 00:01:33.801
é de algumas camadas,
alternando convoluções

00:01:33.834 --> 00:01:34.834
com agregação,

00:01:34.868 --> 00:01:38.467
seguido de algumas camadas
totalmente conectadas no topo.

00:01:38.501 --> 00:01:41.334
O primeiro modelo famoso
com essa arquitetura

00:01:41.367 --> 00:01:42.367
foi o LENET-5,

00:01:42.400 --> 00:01:45.200
projetado por Yan LeCun para
reconhecimento de caracteres

00:01:45.234 --> 00:01:47.033
em 1998.

00:01:47.067 --> 00:01:50.033
As ConvNets modernas,
como a AlexNet,

00:01:50.067 --> 00:01:54.667
que venceu o desafio
da competição ImageNet em 2012,

00:01:54.701 --> 00:01:58.334
usa uma arquitetura parecida,
com alguns detalhes.

00:01:58.367 --> 00:02:02.067
Outra forma interessante
de agregação é a média.

00:02:02.100 --> 00:02:03.834
Em vez de usar o máximo,

00:02:03.868 --> 00:02:06.601
tire a média
da janela de pixels

00:02:06.634 --> 00:02:08.767
em volta de
um local específico.

00:02:08.801 --> 00:02:11.968
É como ter uma visão borrada
e de baixa resolução

00:02:12.000 --> 00:02:14.067
do mapa de características
subjacente.

00:02:14.100 --> 00:02:16.167
Vamos tirar proveito disso.

