WEBVTT
Kind: captions
Language: zh-CN

00:00:00.370 --> 00:00:03.060
现在你已看到简单卷积神经网络是什么样的

00:00:03.060 --> 00:00:05.550
我们可以通过很多方式来改进它

00:00:05.549 --> 00:00:10.330
我们将讨论其中三个：池化、1x1 卷积和

00:00:10.330 --> 00:00:12.960
一个高级点的 Inception 架构

00:00:14.130 --> 00:00:18.109
第一个能更好地降低卷积金字塔中特征图的

00:00:18.109 --> 00:00:20.809
空间维度

00:00:20.809 --> 00:00:25.429
目前为止 我们通过调整步幅 将滤镜每次移动几个像素

00:00:25.429 --> 00:00:27.589
从而降低特征图的尺寸

00:00:27.589 --> 00:00:30.879
这是降低图像采样率的一种非常有效的方法

00:00:30.879 --> 00:00:32.298
它移除了很多信息

00:00:33.619 --> 00:00:36.669
如果我们不采用每两个卷积跳过一个的方法

00:00:36.670 --> 00:00:40.810
而依然执行非常小的步幅 比如说 1

00:00:40.810 --> 00:00:45.030
但是我们通过某种方法把相邻的所有卷积结合在一起

00:00:46.560 --> 00:00:50.790
这种操作就叫做池化 有几种方法可以实现它

00:00:50.789 --> 00:00:52.606
最常见的是最大池化

00:00:52.606 --> 00:00:56.768
在特征图的每一个点 查看它周围很小范围

00:00:56.768 --> 00:01:00.603
的点 计算附近所有点的最大值

00:01:00.603 --> 00:01:03.548
使用最大池化有很多优点

00:01:03.548 --> 00:01:06.426
首先 它不会增加参数数量

00:01:06.426 --> 00:01:08.936
所以你不必担心导致容易过拟合

00:01:08.936 --> 00:01:12.688
其次 它通常会提高模型的准确性

00:01:12.688 --> 00:01:16.813
然而 由于在非常小的步幅下进行卷积

00:01:16.813 --> 00:01:20.123
模型必然需要更多的计算量

00:01:20.123 --> 00:01:23.588
而且你有更多的超参数需要调整

00:01:23.588 --> 00:01:26.331
例如池区尺寸和池化步幅

00:01:26.331 --> 00:01:27.890
它们不必完全一样

00:01:28.920 --> 00:01:33.810
一种典型的卷积神经网络结构为卷积层和最大池化层

00:01:33.810 --> 00:01:37.680
相互交替 然后在最末端连接几层全连接层

00:01:38.689 --> 00:01:42.730
第一个使用这种结构的著名模型是 Lenet-5

00:01:42.730 --> 00:01:47.323
它在 1998 年由 Yann Lecun 设计 用于字母识别

00:01:47.323 --> 00:01:51.524
现代卷积网络 如著名的 AlexNet

00:01:51.525 --> 00:01:55.727
在 2012 年赢得了 ImageNet 物体识别挑战赛

00:01:55.727 --> 00:01:58.609
就使用了一种非常类似的结构

00:01:58.608 --> 00:02:02.108
另一种值得注意的形式是平均池化

00:02:02.108 --> 00:02:03.695
相比于采用最大值

00:02:03.695 --> 00:02:08.990
它使用了特定位置周围的像素的平均值

00:02:08.990 --> 00:02:12.270
它有点像提供了下层特征图的 

00:02:12.270 --> 00:02:13.860
一个低分辨率的视图

00:02:13.860 --> 00:02:15.530
不久后我们将利用它

